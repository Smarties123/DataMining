OCOM5204M – Data mining and text analytics © University of Leeds 1 of 13 Machine Translation: Challenges and Approaches Professor Eric Atwell: Hello, this is Eric Atwell, and this lecture is an introduction to machine translation, including some of the linguistic challenges and some of the approaches in practice to machine translation. For more details of current research methods and algorithms for machine translation, see the chapter on machine translation in the Jurafsky and Martin textbook on speech and language processing. This particular talk is based on some presentation by Professor Nizar Habash. He's professor of computer science at New York University, Abu Dhabi. And he has developed an Arabic computing research laboratory there called the CAMeL lab. So, if you Google Nizar Habash CAMeL lab, you'll find out more details of this. OK, so you probably know about Google Translate. And if you have a piece of text and need it translating... in fact, currently on British television, they have some really good adverts for Google Translate which include speech-to-speech translation. But even via the web, Google offers translations in, as you can see, many, many languages, including several I've never even heard of before. But it can translate between any of these things. So, Google is not interested in developing language models specifically for English, or just for Chinese, or just for Hindi. They only develop... They're developing language models which work in general with text of, not just the Roman alphabet, but many other alphabets too. And they want to have generic models of language which are statistical language models. Just as an aside, another interesting language-related research project, Leeds University is the World Mapper project. And here's their URL. And World Mapper draws maps of the world. But they are transformed by...For each country, taking some figure, some number, which represents some feature in that country. For example, here we have the map of the world where each country is morphed by the number of Arabic speakers in that country. So, in other words, it's a map of the Arabic speaking world. OCOM5204M – Data mining and text analytics © University of Leeds 2 of 13 Focusing on the big lump in the middle, I think, is probably Egypt. And nearby, you can see Saudi Arabia, and Syria, and Jordan, and other Arabic speaking countries. OK, that was an aside. Now let's summarize. What we're going to look at in terms of machine translation, the road map is, first of all, look at some challenges for machine translation. Why is it difficult? Then look at some of the inherent approaches without going into details of these specific algorithms. And then finally, how do we evaluate if a machine translation is good or not? So, why is it difficult? Well, one fairly immediate problem, if you're going to deal with all these different languages, is that they have different character sets. And the characters or spellings aren't necessarily directly as straightforward or phonetic. Or very often, there may be ambiguous spellings. So, there are different ways... A particular combination of letters can mean several different words. Particularly, for example, in Arabic where you typically leave out the vowels. So, there's many different sounds represented by the same sequence of consonants. In other languages, for example, in Chinese, there's the problem of word boundaries. We saw this in an earlier lecture on word modelling and word semantics. There is also, even in English, a particular word. Even if it's ambiguous as a word like bank, it can have several different meanings. And each of these meanings or senses has to be translated into something different in another language. So, the word bank, in English, can either be a financial bank or the sides of a river... riverbank. And these are translated to two different words in Arabic. And even words which you might not think are ambiguous can involve two different translations into another language. So, "eat”, you think, just means to consume or to put things in your mouth and chew and swallow them. Whereas, in German, there are two different senses of eat. “Essen” is a human eating, and “fressen” is an animal eating. So, in the German language, there is a distinct difference between human eating and animal eating, and if you're doing a machine translation system, you have to get this right. OK, apart from just the words meaning several things or different things. Different senses. there's also a problem with morphology. That is how a word is divided up into pieces or morphemes. In English, this isn't very difficult. A verb may have a past tense. So, "kill”, the past tense is "killed", "write”, the past tense is "written", "do”, the past tense is "done". And you see the ending is different for each of these words. OCOM5204M – Data mining and text analytics © University of Leeds 3 of 13 In Arabic, often, an affix...What might be an affix in English becomes an infix or even two bits of things. And tokenization can be even more complicated. Sometimes what is a single word in one language becomes several words in another language. So, for example, "and the cars" in English, it is essentially two words in Arabic rather than three words. And we see in French. Well, in English, the "cars”, "cars" is plural, and that's noted by "s" at the end of car. In the French, "et les voitures". "Et" means "and”. "The" is translated into "les". And "les" has to have an "s" on the end to note it's a plural verb. And "voiture" is the French for car, and it also has to have an "s" to show it's a plural verb. So, sometimes the plural markers have to go on more than just one word. Then you can have divergences in that the way that the meaning is divided up is different in different languages. So, in English, "I am not here" is just four words. In Arabic, the same phrase, "I am not here" is equivalent to two words. The first word means "I am not”, and the second word is "here". Or in French, you'd say "je ne suis pas ici". And the "ne pas" mean "not", and you have "am" in between the two. So, it's "I not am not here”. And this is quite common. Even more complicatedly, what may be a verb in English, like "John swam across the river quickly”, if you translate it into Spanish, well, you'd actually say something like "John crossed fast the river swimming”. So, the verb becomes "crossing" rather than "swimming”. Or in Arabic, it would be something like "Sped John crossing the river swimming”, so, the verb becomes "speeding" rather than "crossing" or "swimming”. And in other languages, it's different things. In Russian, it's "John quickly cross swam river”, so "cross swimming" becomes the verb. So, what counts is the verb is different in different languages for a sentence translated. So, that's some of the linguistic problems. Another challenge is you have to have some resources if you're going to train machine learning models for machine translation. You obviously have to have a corpus, a data set. You might use something like WebBootCat to collect a web-based corpus. And you also have to have a dictionary, or list of words in the language. But actually, you don't just want to have. If you do build an English to French machine translation system. You're not good just to having a corpus for English and a dictionary of English and a separate corpus of French and a dictionary for French. You really need a parallel corpus with a source. OCOM5204M – Data mining and text analytics © University of Leeds 4 of 13 That's the English... And the target...That's the French... aligned, so you can work out this English sentence translates into that French sentence, or this English word. You have to learn that a line dictionary says for each English word, what is the equivalent French word? And this is a problem because you can't just go to the web and find web pages in English with their French translations as easily as you can just find English web pages. But it's particularly difficult for languages which have few resources at all, like non-European languages, like Amharic from Africa or Bengali from Asia. There's many millions of speakers of these languages. Native speakers... but there's very few webpages or parallel corpora. You may be able to get Bengali web pages, but getting English Bengali parallel pages, or Bengali Amharic parallel web pages is very hard. That's some of the challenges for machine translation. Now, let's look at a very broad sense of, how is it done? What are the approaches? Well, there's this idea of a machine translation mountain or pyramid. The idea is you have to start with a source. That's what you'll say, if you're translating from English to French, you have source words. You'd have a source grammar of English and the meanings of English sentences. These have to be translated into the target. If it's into English to French, it would be French meanings, the French grammar, and the French words. So, you analyze or have analysis of the English source sentence, and then you generate the target French sentence. A simple way to do that is just word for word gisting. Taking each of the words, and…Here's an example from Spanish. "Sobre la base de dichas experiencias se establecio en 1988 una metodologia". See, I don't speak Spanish very well, so I just said 1988. It isn't the Spanish word. OK, here we have a word for word translation, and in English, it becomes "Envelope her basis out speak experiences then settle at 1988 one methodology”. And that's not a well-formed English sentence. But maybe you can work out the gist... what it sort of means. So, if you ask a proper translator, a human translator, they would say, it means something like, on the basis of his experiences, a methodology was arrived at in 1988. OCOM5204M – Data mining and text analytics © University of Leeds 5 of 13 And maybe, from a word for word translation, you can work that out, because it's the same words, but moved around a bit. So, long as you know what the rules are for moving things around, then that will work. We'll have a look at a better example of that in a minute. So, that's word for word. It doesn't work perfectly. Maybe what you should be doing is taking the grammatical structures of the source and transferring them into the grammatical structures of the target. So, here's a simple example. In Spanish, you might say x. Whenever x and y are variables. "Puso mantequilla en" And that means "Put butter on”. But in English, you can say, "He buttered his toast”, for example. So, there we have a phrase, a structured phrase, in Spanish, translating into a single word or verb in English. So, that's the idea of a transfer lexicon. So, if you have trouble with this, you need lots and lots of handwritten examples to do that. Maybe what you actually want to do is work out the meaning. Some representation of the semantics, or meaning...of the English sentence, and then convert that into the meaning of a French sentence, and then generate a French sentence. There have been many attempts to do this sort of thing. These are fairly complicated graph structures representing meanings or phrases, like "John broke into the room”, and their equivalents in other languages. This gets fairly complicated. So, in fact, that didn't really work out, because trying to work out hand-drawn graph structures for individual meanings of sentences never works very well. What you really want to do is, if you can get hold of a large corpus, a large training set, then you could apply machine learning to it. In this case, it's a parallel corpus...a corpus with lots of English sentences and for each English sentence, the French equivalent. And from that, you can work out, for each English word, what is the French equivalent word, and what are the rules. The mapping between them. So, you really need to have dictionaries, or lexicons. You have to have a parallel corpus, or parallel dictionary. You also have to have a dictionary of phrases and a dictionary of meanings for all these things. So, lexicon is another word for dictionary. So, how can you work out how to translate words and phrases from one language into another? Well, a nice example, if you Google this, is Giza plus plus. Giza is a place in Egypt. It's also a statistical machine translation toolkit used to train word alignments. It uses expectation maximization. It essentially tries to find out, what's the most likely combination? OCOM5204M – Data mining and text analytics © University of Leeds 6 of 13 So, here we have, as an example, an English sentence, "Mary did not slap the green witch”. And the Spanish translation is "Maria no dio una bofetada a la bruja verde". And here we have the actual mapping. "Mary" is "Maria". "Did not" is "no". "Slap" is "dio una bofetada"… "give a hit”, if you like. "The" is "la" "Green" is "verde" and "witch" is "bruja". We see the green witch. The adjective noun is converted into noun adjective in Spanish. So, you have to work that out. And if you get enough examples, you can do that. IBM developed a model for basically gisting word for word translation, but they worked out, as we saw in the initial example, that you actually have to change the order of some of the words. They came up with various sort of hand-drawn rules for doing this. So, for example, "Mary did not slap the green witch”. First of all, there's a fertility rule, which says that some words, like "slap”, have to be converted into longer phrases, like "Mary not slap, slap, slap the green witch”. And then there's null insertion. After slapping... in English, you slap something, whereas in Spanish, you slap to the something, or you give a slap to something. And then you have a translation, translating each word for word, "Maria”, or "Mary”, in this case, "no daba una fotefada a la verde bruja". That's more almost Spanish. Finally, you have this distortion rule, that adjective noun becomes noun adjective. To swap them around. And you end up with "Mary”, or "Maria", "no daba una botefada a la bruja verde". In other words, you can do word for word translation, with some extra steps in, and that gives you a reasonable translation for sentences which are fairly straightforward for languages which are quite similar. So, Spanish and English, or German, or French, or most other European languages, have more or less the same sort of grammar, and therefore that works. It doesn't work so well for English to Arabic, or English to Chinese or Japanese, because they are very different. So, another way around this is what's called phrase based statistical machine translation. We've already discovered that sometimes, phrases like "green witch”, you can't just simply do word for word. You have to change things around. So, what you can do is try to look for phrases of several words, rather than just individual words. So, you might notice, for example, here's a German sentence. "Morgen fliege ich nach Kanada zur konferenz". And the mapping for this..."Morgen" is tomorrow, "fliege ich", is "I will fly”, so that has to be swapped around. And "nach Kanada zur OCOM5204M – Data mining and text analytics © University of Leeds 7 of 13 konferenz" is to the conference in Canada. You don't say "I will fly in Canada to the conference”. You have to say, "I will fly to the conference in Canada”. So, there's always swapping things around, but the things that are moved around are whole phrases, not just individual words. So, if you can work out where the phrases are, then you can build this machine translation system much better. So, how do we work out what the phrases are? Well, if you have. Going back to our example, "Mary did not slap the green witch”. If you know more or less what the word for word translations are, then you can infer larger phrases that might be translated. So, for example, "slap" is "bofetada". And "the" is "la". But there's this "a" in there which is missing. So, maybe you can combine these together and say, if you're not sure what "slap the" is, then it should be "dio bofetada a la". Similarly, you can infer large... So, "green witch" becomes "bruja verde" so you've got to swap them around. But you've just always learned that "green witch" is always "bruja verde". And there's no swapping to do. You just learn the whole phrase in English goes into a whole phrase in Spanish. And he's got bigger and bigger phrases. "Mary did not slap" becomes "Maria no dio una bofetada". "The green witch" becomes "a la bruja verde". Or perhaps the entire sentence. So, that's one way of doing phrase based machine translation. Many, many mappings can handle non-compositional phrases. You can get a whole complicated phrase, or even a whole sentence, can be translated in one go, and therefore you don't have to worry about moving things around. And also, you can capture things like "interest" being ambiguous, between monetary interest or "I like you" type of interest. But if you just translate "interest rate" differently from "interest in”, then you don't have to know that interest has got two different senses. "Interest rate" is different from "interest in”. They're no longer a single spelling with two different senses. They've got two different spellings with two different senses. And furthermore, just as we saw with earlier work by Banko and Brill, who discovered that if you get more and more data, then the classifiers work better and better. OCOM5204M – Data mining and text analytics © University of Leeds 8 of 13 In the same way, the more data you have, the longer learned phrases you can learn, the more long phrases you can learn. And then eventually, you learn to translate whole phrases and sentences, and you don't have to use word for word mapping anymore. That's basically how Google Translate works. They started off doing word for word translations. Over time... if you go to Google Translate now, it invites you to say, is this a good translation? If not, can you suggest a better one? And from all of these better ones, it's been able to work out more and more longer phrases that it learns. So, that's machine translation approaches. As I said, if you want to look in more detail about algorithms, then have a look at the chapter in Jurafsky and Martin. They've got a lot more about current research into algorithms for machine translation, which I am...in this module, it's not enough time or space to go into details. But if you wanted to, for example, for your project, you could well try out this Giza system or some of the other approaches that are in the Jurafsky and Martin textbook. Finally, I want to say, how do you know if it's any good? So, a machine translation system comes to you, or, what often happens here at Leeds University, we have a Centre for Translation Studies, and companies will come along and say, we're using this machine translation system. Is it any good? Or, we're trying to decide, should we buy x or y. Which one is better? And it's pretty much an art, or at least it used to be. Well, one thing, as with most IT systems, you can measure things like, is the interface nice? How does it scale? How does it interact with your existing other systems? Things like that. How slow is it? How fast is it? But maybe what you're actually interested in, in terms of how good is it, is, is it actually correctly translating? Is the translation correct, regardless of how long it takes or how expensive it is? And that's where we want to... that's where most research in machine translation evaluation goes into, rather than just evaluation of IT systems. Specifically, about machine translation, it's, how good is the translation? And this often involves using people to do it, rather than…humans can do it very well. They can look at a translation output and say, this is good, or I can see some mistakes in it. But they can be quite slow. Whereas it would be nice to be able to do this automatically, or semi automatically, because you can speed it up, and you can also evaluate a lot more outputs. OCOM5204M – Data mining and text analytics © University of Leeds 9 of 13 So, the way that it's done by humans is, typically, you apply the machine translation system to a test set, and it produces output. Say, 100 sentences in English are input, and outcome 100 French sentences. And you give these to some people, some human translators, and they rank them in terms of fidelity or accuracy. Standards ranges from 5 to 1. 5 means it's pretty much perfect, it might need some minor corrections, whereas 1 means it's completely missed it. The contents of the original sentence are not conveyed. And in between the two…it's not just a binary thing. You can get 2, 3, or 4, as well. That's in terms of, is it getting the meaning across? Is it capturing... is the French output sentence, does it actually mean the same as the English input sentence? However, there's another thing to measure, and that's called fluency, or intelligibility. Is the output, the French sentence actually clear, good grammar, good sentence structure? Does it sound natural? Because if it's not natural, then it can hide the meaning, and at worst, you can have a sentence, a word for word translation, for example, which literally does capture the meaning of the English sentence, but it doesn't make any sense. Its meaning is absolutely unclear to an ordinary French speaker. So, there's two measures. There's fidelity, or accuracy, and separately, there's fluency, or intelligibility. So, machine translation evaluation measures both of these things. And it can be quite expensive to get. Typically, you want to have two or three people, not just one person, measuring the output for each sentence, and coming up with two scores. One for accuracy and one for intelligibility, and then somehow combining these to give a score overall. That's quite time consuming. So, maybe another approach is to semi-automate it. IBM came up with a metric called BLEU. "Bleu" is the French word for "blue”, and blue is what IBM is, so that's why they called it that, I suppose. But it also stands for the bilingual evaluation understudy. Understudy is... In a theatre, it's someone who stands in if the actual actor or actress is ill or is unable to play. So, BLEU helps the humans. It doesn't actually completely take over for humans. What it does is, given an English sentence, and it's got to be translated by humans into French, there's never one single perfect translation. So, what you do is you get three translators, or several translators, to translate each sentence, and you come up with a bank of translations for each of the inputs. Then, the BLEU metric calculates a score for that. OCOM5204M – Data mining and text analytics © University of Leeds 10 of 13 It's particularly inexpensive and language independent. The score works reasonably well. So, it's a way of generating a score, and the nice thing is the score correlates with human evaluation. So, if there's two machine translation systems, and BLEU gives one a score of 56 and the other score 49, that means that the 56 one is better. And typically, people will agree that the one that the BLEU says is better is actually the better one. People may have other...not a straightforward score, but they'll have more subjective opinions about it. It compares the machine translation against several human translations to give a standardized score. Let's just give an example of this. Let's say, for a particular French sentence...I can't remember what the French sentence is. The machine translation output was "Colorless green ideas sleep furiously”. Anybody who's listening to this, who's read the works of Noam Chomsky...Noam Chomsky is a very famous linguistics researcher, and he came up with a book called Syntactic Structures where he used this as an example of a sentence which, in English, is grammatically correct, but doesn't seem to mean anything. Anyway, here's the sentence... "colorless green ideas sleep furiously”. Now, the humans have taken the French sentence, and the first one translated it "All doll jade ideas sleep irately”. And a second human translated the French sentence into "Drab emerald concepts sleep furiously”. And the third translator took the French sentence and translate into "Colorless immature thoughts nap angrily”. Now, we see that this is a made up example, just to make it easier to explain the mathematics, but we see that none of the translations are the same. All three are done by humans, therefore, should be OK. But the machine translation output isn't the same as any of them. So, if you want to say machine translation is good if it's the same as a human's... Well, it gets a 0 score. So, that's no good. And furthermore, the humans don't agree. So, as far as human 2 and human 3 are concerned, human 1 got it wrong. So, requiring an exact match is unreasonable. What you can do is look at bits...That's unigrams and bigrams, individual words and pairs of words, to see how well they are translated. And that's what the BLEU metric does. First of all, there's a unigram score, or unigram precision. In the test sentence, how many of the words, the individual words, appear in any of the outputs, or the gold standard references by the human translators? And "colorless" appears in one of them. "Ideas" appears in another one. "Sleep" OCOM5204M – Data mining and text analytics © University of Leeds 11 of 13 appears in two of them. "Sleep" does appear there. "Furiously" is in one. "Sleep" appears in twice, but it doesn't get a double score. But four out of the five words in the test sentence from the machine translation output are included, so that's a unigram precision of 4 out of 5. And then you also look at the bigram position. Well, "Colorless green ideas sleep furiously" maps onto four bigrams... "colorless green”, "green ideas”, "ideas sleep”, and "sleep furiously”. And "ideas sleep" does appear in one of the translations, and "sleep furiously" does appear in another translation. Therefore, it gets the bigram score of 2 out of 4. And then the BLEU score is simply, multiply together all the ngram scores and take the n-th root of the ngram scores. In this case, we're just looking at unigram and bigram. You can look at trigrams as well, but the simple model is just to take the unigram score and the bigram score. That's 0.8 times 0.5, and the square root of that is 0.6325, or 63.25%. And that's the score. You get a score of 63 for this machine translation system, and then you try another machine translation system, and whatever its output is may get another score...Let's say 52...And that tells you that this machine translation system is better than the other one. It's not that the humans will come up with a score of 63, but rather there's a correlation between the ranking given by the BLEU score and the ranking given by human evaluators. But the human evaluators will take a lot longer to make this decision, whereas BLEU gives you a metric which works, and it seems to be more or less correlated to humans but does it consistently and efficiently. OK, then, to summarize, machine translation is difficult for various reasons. Languages have got different writing systems. The words are divided up differently in morphology. That is, dividing up words into segments or morphemes is different. Even for a whole sentence, it can be divided up into different sentences or words differently in different languages, and the ordering of words is different in different languages. Also, there's the point of needing resources. You don't just need to collect a corpus using Sketch Engine or WebBootCat. You need a parallel corpus. You need to have...For a pair of translations for English to French, so you have to have an English corpus, and each sentence, its French translation. And that could be quite difficult. OCOM5204M – Data mining and text analytics © University of Leeds 12 of 13 There are sources like the United Nations, where every document is translated into English, French, Chinese, Hindi, Arabic. Something else? No, I think that was it. That means there is a huge parallel corpus available for those languages of United Nations legal texts. But if you want to have other languages or other domains, other subject areas, then it's difficult. You also need dictionaries, and not just dictionaries as lists of words, but translation dictionaries. You have to have an Arabic to English and an English to Arabic translation dictionary if you want to translate between those two languages. You also need a bank of other tools, like segmenters and parses and things. So, the machine translation approaches, there are simple word for word mapping. To do it a bit more sophisticatedly, you have to have word for word plus rules for messing about the order, reordering things in special cases. There are also transfer models, where you transfer entire phrases rather than just individual words, and you can build this up over time. You have a translation memory of things you've translated before, which includes not just the words, but the phrases and the sentences. You can try an interlingual where you have some sort of semantic representation of the input sentence, and that's mapped onto the semantic representation in the target language, and then you generate the target sentence. That's maybe sort of a neural network type approach, as an alternative. And finally, we have this issue of evaluating. How do you decide if a machine translation system is good? Well, it's not just an accuracy score, but there's also fluency that has to be taken into account. These are two separate things, a bit like precision and recall in measuring how good a classifier is. So, accuracy is how much of the original meaning is carried across in the output translation, and fluency is, given the output translation, can an ordinary native speaker of that language actually understand it? And you may have very accurate, in terms of word for word translations, which are very difficult to understand and very low fluency. Doing this by humans, getting people to do it, is quite time consuming and therefore costly. IBM BLEU is another method. And there are other ones too, but BLEU is the famous one, the first one to come up with this automated method or semi-automated method. OCOM5204M – Data mining and text analytics © University of Leeds 13 of 13 If you get a test set of sentences translated by humans, and each one has to be translated by several translators, say, three or so, then you can count the unigrams and bigrams overlap between the translation output and the human translations, and then use this to calculate this score. And this score correlates very well with human evaluations of machine translation systems more generally. OK. As I said, if you want more details of the algorithms, then go to your Jurafsky and Martin textbook. In fact, please do read that textbook, because there may be questions on that in the exam coming up, or the test coming up at the end of the module. Enjoy, and try out some of these tools yourself, if you want to. And that's it for me for now. Bye.
--------------------------------------------------------------------------------------------------------------------
OCOM5204M – Data mining and text analytics © University of Leeds 1 of 10 Information Extraction: Extracting Named Entities and Relations from Text Professor Eric Atwell: Hello. This is Eric Atwell, and I'm going to today talk about information extraction. That is, extracting named entities and relations from a text. And this lecture is based on a presentation by Diana Maynard. She's a researcher at the Natural Language Processing Group at University of Sheffield in the UK. Sheffield's about half an hour to an hour's drive from Leeds, depending on traffic. And in case you Google Diana Maynard, you may come across her, as well as being a chief researcher, she's also involved in ballroom dancing. So, that's something else to look at. OK. So, what we're going to talk about-- first of all, what is information extraction and some applications of information extraction. And then we're going to look at two different approaches to building information extraction systems. We've seen lots of machine learning examples in artificial intelligence. But another way of doing it is a knowledge engineering approach to the experts to devise some rules. And we're going to look at how you go about developing our rule-based named entity recognition system, and as an example from Sheffield, the MUSE, MUlti-Source Entity recognition system, to finish off some ideas about ongoing research and information extraction. So, first of all, what is information extraction? Well, you may remember, we had a lecture on named entity recognition, recognizing in a text that New York is a named entity, not just two words. It's information retrieval like Google search, and you give it some keywords or queries, and the information retrieval system pulls documents which seem to match your keywords from a large text collection. And you've probably used information retrieval in terms of searching the web via Google or Yahoo! or some other search engine. So, a Google search is one example of information retrieval. And, technically, the thing that's pulled are still called documents in information retrieval theory, even though there could be web pages or something else. But you must then still have to read or analyse the documents to find the answer to your questions, to find what it is you actually want. Information extraction goes a bit further in that given a document, or a set of documents, it pulls facts and structured information. Essentially, it pulls out the named entities and the relations between those named entities from the contents of a large text collection, such as a corpus or corpora. You then don't have to read all the documents. You just have to analyse the facts it has pulled out. And you also get a link back to the original documents, so you can go back and check that it got it right. So, why would you want to do this? Well, with traditional information retrieval query engines, getting out the facts can be quite difficult. For example, if I wanted to find out where has the queen visited in the last year or which places on the East Coast of the United States have had cases of West Nile virus-- well, first of all, you've got to think what are the search terms to you to try to get the web OCOM5204M – Data mining and text analytics © University of Leeds 2 of 10 pages. Queen, visiting, 2021-- depending on what last year is. And we probably want the queen of England, Queen Elizabeth. And you have to be careful with Queen, because there are various other queens you might find out. So, getting the search terms is quite difficult. Information extraction would hopefully return information in a structured way to get information like the queen of England as an entity. And other names for her might be Elizabeth or Queen Elizabeth. Information retrieval would simply return the documents containing the information, hopefully somewhere if you're lucky. So, for various types of search, information extraction can be a better alternative to this information retrieval. Information extraction returns knowledge in some way at a deeper level for information retrieval. You can use information extraction to pull out the entities and relations and construct a database, or knowledge base if you want to call it that, which contains the information in the documents in a more structured way. And it can be linked back to the documents as an alternative search. So, you can still search the documents. You can find the entities. And if you're not sure you've got the right answer, you can go back to the documents to read them. So, even if the results of information extraction aren't always accurate entirely, they can still be valuable. Because you still can check them in the original text. So, it's particularly useful for access to news, to identify in news stories the major events, the major players, the entities, and the relationships between them, for example, foreign affairs or business news. So, if you wanted to find out what companies buying which other company, you can find out entities like companies and relationships between them, such as buying. It's also very useful for research in scientific areas, particularly in medicine, and pharmacology, and genomics, where in the subfield there are lots of very specialized terms for specialized sorts of entities and specialized sorts of relations. So, being able to extract names of particular pharmacological entities, these drugs and what they do to each other or what they do to diseases, can be useful. Let's look at a few simple examples to get you an idea. So, one developed at Sheffield was HSE, the Health and Safety Information Extraction System. So, this aims to look at company reports about health and safety. So, you may not be aware but every company including Leeds University, which is a sort of company, has to report to a health and safety inspector about health and safety issues. And then every year the health and safety inspector may come around and ask to look at the documents, and ask questions like, how many members of staff have died or had accidents in the last year. Hopefully, at Leeds University, not many deaths and not so many accidents. Or another question-- is there someone responsible for health and safety in the university, or what measures have been put into place to improve health and safety in the workplace. Those are the sorts of questions that an inspector might want to ask. And if you just have information retrieval-- well, if you haven't got any system at all, you have to read the books to find the answer. Or information retrieval wouldn't be particularly helpful because it might return which documents have information about this in it, but the documents could be quite large. You still have to read the documents. On the other hand, what HSE does is it identifies sentences about health and safety issues or entities, and extracts them, and populates a database with the entities and the relationships between them, so that hopefully you can query the database or knowledge base rather than having to read OCOM5204M – Data mining and text analytics © University of Leeds 3 of 10 everything. OK. So, here's another example, KIM, the Kibbutz Information Management System. So, if you don't, know a kibbutz is a collective cooperative farm commune in Israel. And Israel is well known as a place where there may be terrorist attacks due to unfortunate circumstances. So, you can ask the Kibbutz Information System queries, and the queries are like English sentences. So, what they've done is they've taken lots of newspaper stories and extracted anything to do with kibbutzes, and entities around kibbutzes and relationships to do with kibbutzes and put them into a database. And then, being a database, you can query it using SQL or some sort of structured query language. Because most ordinary people don't really understand how to write SQL queries, there is an interface which is sort of English sentence-like, or sort of quasi-mathematical. You can search for patterns where x is a-- and then you can specify, for example, x is a person, which name is known or is unknown. And x, furthermore, is involved in y, where y is an event which name contains kibbutz attack. And y took place in z, where z is a country which name is exactly equal to Israel. So, this is a roundabout way of saying, I want to find examples of a person x-- we don't know the name of the person. And this person was involved in an event named a kibbutz attack. And the kibbutz attack took place in a country called Israel. In other words, I want to find people-- I want to find persons involved in kibbutz attacks in Israel. OK? I want to find persons involved in kibbutz attacks in Israel. It's a bit more complicated. So, the query language isn't part of the information extraction mechanism. It's just the interface to the information extraction database. But here's some examples of replies, and these are newspaper stories. For example, the top one is Israel Kibbutz Attack Suspect Arrested, where probably the suspect is a person who's likely to be involved in a kibbutz attack in Israel. So, that one's right. If you're not sure, you can click on the article and read it. So, as well as having the answer, you also have the evidence for the answer, which if you're not sure you can go and check the evidence. OK. Here's another example. This was Alias-i-- I think it's a New York company-- developed something called the Threat Tracker at the time of the Americans invading Iraq. You might think a threat tracker is some sort of satellite-based tracking system, which can see things which are threats. Actually, this is just tracking threats within text. So, it's not using vision or image processing. It's text analysis. In their case, the threat-- the entities are dangerous entities such as Huda Ammash. So, Huda Ammash was one of the most wanted terrorist suspects when they were attacking. So, the Americans invaded Iraq, and they gave every soldier a pack of cards. And on the back of each card was a photo of one of the terrorists they wanted to catch. That way they could recognize them. OK. So, here we have underlined or enbolded in yellow the mentions of the entity Huda Ammash found by the threat tracker. So, we have Mrs. Anthrax. That was her name, because she was supposed to be developing anthrax, a deadly disease. And we have Mrs. Anthrax again, and then we have Huda Salih Mahdi Ammash, which is her full name. And she was dubbed Mrs. Anthrax. And then later on it says Ammash. That's just her surname. And then later we have 5 of Hearts. 5 of Hearts was the card where her photo was on the back. 5 of Hearts was another sort of name for her. OCOM5204M – Data mining and text analytics © University of Leeds 4 of 10 And then, finally, we have it described Her as a weapons of mass destruction suspect. So, Her is also indicating it. These are different references to the same entity. So, named entity recognition is identification of proper names and other references to entities in text and their classification into a set of categories of interest. So, not just that you found Huda Ammash, but also that she is a person. Other entity classes might be organizations like companies or committees or whatever, locations such as cities or countries, dates and times, or other types as well depending on what sort of task you have. So, if you're doing medical text recognition you might have medicine entities or disease entities. So, named entity recognition is important as a foundation to build more complex information extraction. If you want to find out relations between named entities then first of all, we have to identify the entities and then you can go on to find out the relationships between them. And this provides tracking in the text sense, ontologies, and scenarios. So, you might have tracking co-reference. Dr. Anthrax, Huda, and she all refer or co-refer to the same entity. You might have ontology relations like persons, events, and countries are all types of entities within an ontology and they're related in various ways. And the relations could be also from a tech. Like Dr. Head is an entity and Shiny Rocket Corporation is an entity, and in the sentence, "Dr. Head became the new director of Shiny Rocket Corporation," then the relationship is director, that sort of thing. So, how do you build systems for recognizing this? Well, on the rest of the AI/MLC you've come across machine learning quite a lot. Machine learning uses statistical language modelling or machine learning. You have to have-- you don't-- you have to have people who understand machine learning. They don't necessarily have to understand the domain. If you're doing machine learning from medical texts you don't have to be a medical doctor. It does require a large amount of training data annotated with the targets. So, someone has to go through the text and label this is a person, this is a company and so on. And typically machine learning models, particularly deep learning models, require a lot of computing processor and memory. Furthermore, if you want to make some changes-- for example, you're not just interested in people and companies but you're also interested in universities or public organizations separate from companies, and that means reannotation of the entire training corpus. You have to go back and everything that was labelled a company maybe have to be relabelled as a university because you're separating out industry companies from non-commercial companies or something. OK, so that's the pros and cons of machine learning. Well, we'll have some of the disadvantages of machine learning. You need a lot of compute and a lot of effort to collect the data and annotate the data. The knowledge engineering approach, this used to be much more popular before powerful machine learning was actually feasible and this is to come up with hand-crafted rule-based systems. This doesn't just apply for information extraction but to other tasks. If you remember, part of speech tagging I mentioned that at Helsinki University you could spend three years on a PhD just developing a rule-based system for part of speech tagging the language of choice, the English, Finnish, Russian, or whatever language you happen to like. OCOM5204M – Data mining and text analytics © University of Leeds 5 of 10 And they didn't need to have a lot of data to do this. You have to be experienced in the topic area. You have to know something about that the task. It requires-- you have to be smart. It requires human intuition to figure it out. You only need to have a small amount of example training data to figure out possible rules for deciding this is a noun and this is a verb. You don't need thousands of nouns and thousands of verbs, just a few of them to see typical contexts. And the same thing for information extraction. You may not require high-performance computing. It may work on your laptop. But development time can be time-consuming. And if you want to change-- for example, you don't just want to identify companies but you want to separate out industry companies from non-profit companies then that may mean you don't have to suddenly have to go back to the old data set but you do have to check all the rules you've got so far. And the rule set can be quite large. OK, so that's a knowledge engineering approach, and I'm going to-- because you've had lots of examples of machine learning in other parts of the course I'm going to focus on an example which applies the knowledge engineering approach just to show how it's done. So, let's say first of all you want to recognize the named entities. So, why is this difficult? Well, the real problem in all language analysis is ambiguity. First of all, for a particular named entity you can have different ways of referring to it. So, John Smith, Mr. Smith, and John all refer to the same entity or can refer to the same entity. So, that's many names for one entity. The other sort of ambiguity is one name could be several different entities. For example, John Smith, I just said that was one person, but actually there may be several people called John Smith, or even worse, there may be a company called John Smith. If ever you come to Leeds and then you drive from Leeds to York you'll pass the John Smith Brewery where they make John Smith Beer. So, John Smith is the name of a company in Yorkshire. OK, another example. June-- when I go to shopping at my local Lidl supermarket there's June behind the checkout, and that's different from June the month. Or Washington, another example. I'm told there's a famous American called Washington, I've not learned much about it-- Denzel Washington, I think-- whereas on the other hand, I do know Washington. I've visited. There's a town in the North of England near to Newcastle. So, there's-- it can either be a location or a person. 1945, that's ambiguous as that could be a date, the year that the Second World War ended, or it could be quarter to 8:00 in the evening, about time to have dinner. So, ambiguity is very-- even common, ordinary words may be ambiguous. So, a word like may, because it's in lowercase we think it's just I may like you, but typically you can't be confident that capital letters are always going to be there so this could be the proper noun, May, which is the month May. OK, that's just individual words, and within a document the document structure and style and even the particular domain of a document or the genre of a type of a document can give additional information. Punctuation, spelling, spacing, and formatting are all issues to take into account. So, for example, if I come across a piece of text like Department of Computing and Maths, Manchester Metropolitan University, Manchester, United Kingdom, I as a human immediately recognize this as an address that you might get at the top of a letter. So, somehow the information OCOM5204M – Data mining and text analytics © University of Leeds 6 of 10 extraction system, the named entity system has to recognize that this is a single entity and it's hierarchical and it's the address of the computing department, which is within Manchester Metropolitan University, which is part of Manchester, which is part of the United Kingdom. Or a second example, when we have, "Tell me more about Leonardo da Vinci," but there's a greaterthan sign and I as a human know that that greater-than sign means it's probably part of an email, it's a repeat of a previous email. So, there's additional information from the punctuation or the layout which you would have to know about. Having given an idea of some of the challenges, how do you do named entity recognition? Well, the simplest baseline approach is simply to have lists. The system recognizes entities stored in its lists and the lists are called gazetteers. This is from tradition. A gazetteer used to be a list of place names or person names. This is nice and simple because you basically go through a text. For every word you look it up in the gazetteer. If you find it then it is a named entity. If you don't find it it's not a named entity. Nice and easy. And it's an easy-to-read target for a new topic, a new language, a new domain. You just have to create a new list. The disadvantage? Well, there are several, one of which is that first of all, you have to collect the lists in the first place and maintain them. So, if you're doing pharmaceutical named entity recognition then you have to make sure your list of pharmaceutical entities is up-to-date. Also, it can't deal with this ambiguity problem that we saw. It can't work out that John Smith and John are the same entity, or it can't work out that John Smith could be a person, or it could be a company. So, what can you do on top of just having these lists or gazetteers? The first thing you can do is to start looking at context. There may be internal evidence within the unnamed entity. If you come across, for example, a word starting with-- a capitalized word starting with a capital letter followed by City or Forest or Centre or River, for example, Sherwood Forest, then that is probably a named entity, and it's probably a geographical location. It doesn't always work. For example, Nottingham Forest-- anybody coming to England might know this-- is the name of a football club. It's not-- there isn't actually any forest left near Nottingham. They've cut it all down. Although Sherwood Forest is very near to Nottingham. Another sort of rule is a capital letter at the start of a word followed by Street or Boulevard or Avenue or Crescent or Road is probably a named entity which is a location. For example, Portobello Street is a famous street or road in London. Again, this doesn't always work. For example, it's Christmas now and I've got lots of Quality Street. Quality Street is not a location. Quality Street is a type of chocolate sweets. So, Quality Street is commonly eaten around Christmas. So, that works most of the time. There are exceptions where it doesn't work. There's also-- ambiguity is again a problem. We saw particularly if you're looking for the capital letter at the start of a word, that is something that works reasonably well in English. Other languages like Arabic don't have capital letters, or Hindi doesn't have capital letters so that's not going to work. And even for English it's not always the case that the word starting with a capital letter is significant as a name because we do have this rather awkward rule that the first word of every sentence has to OCOM5204M – Data mining and text analytics © University of Leeds 7 of 10 start with a capital letter. So, if a sentence starts "All-American bank," that could be a named entity, whereas if it starts "All state police" then the entity is actually "State Police," not "All-State Police." Another problem is if you have sequences of these like John F. Kennedy, that could be the airport in-- New York? Yes, it is John F. Kennedy Airport I've been to, or it could be the name of a president in America. Or Philip Morris looks like a person's name but is also the name of a company. You also have structural ambiguity that company names often include words like "and" and "of" in them. So, "Cable and Wireless" is quite a big company, whereas "Microsoft and Dell" is two companies, not one. Or a message from "City Hospital for John Smith," there's entities "City Hospital" and "John Smith," whereas you can have a "Center for Computational Linguistics," and that is one entity. So, that's complicated. So, even if you have sequences of words, you then have to start looking at contexts beyond the sequences of words to help disambiguate. So, for example, "David Walton" and "Goldman Sachs" both look like names of people or names of companies, we're not quite sure. But if you come across "David Walton of Goldman Sachs" then you know that x of y implies x belongs to y and-- well, that's probably a person, David Walton, belonging to the company Goldman Sachs. So, we have person of organization. And you can then-- you can generate rules like this. These are semantic patterns. And how do you do this? Well, you use a quick concordance. For example, if you remember in Sketch Engine you can look for a word like "David" and then look at words which appear around it to give you co-concordances or context around that, and if you find repeated contextual patterns then that gives you a clue to possible patterns. Hence, you've got out these patterns and you can search for more patterns including more of these things and just repeat over and over again. So, these are the sort of patterns that you might extract. If you search using a concordance you might find a person followed by the word "owns" followed by another entity. It's likely that the first entity is a person and the second is some amount of money. Or "x joined y," x is likely to be a person and y is likely to be an organization and so on. And this doesn't just find you the classes or types of entities, but it also tells you the relations between the entities. So, this finding-- if it's a pattern "person earns money," if you find "John Smith earns 15 pounds," then you've identified "John Smith" as a person as opposed to a company, and "15 pounds" as a money entity. And you've also found the relationship between these two is the earning relationship. So, these semantic patterns find the entity types and the relation types. So, that's, in a nutshell, how it's done in the knowledge engineering or rule-based system approach. You can build similar things using machine learning but for machine learning you actually have to have hundreds of examples of each type. So, you have to spend a lot of time getting hold of a corpus and getting people to manually mark up the named entities, the types of the entities, and the relationship between the entities, and then you can try to learn from the corpus. So, you can imagine doing the markup up can take a lot longer than just building the rules. Let's look at MUSE. This is an example of the multi-source entity recognition system built at Sheffield University using their GATE, General Architecture for Text Engineering. And to find out more about ways you can go to the GATE website, gate.ac.uk. Notice it's not gate.co.uk but gate.ac. The ac is Academic Community. OCOM5204M – Data mining and text analytics © University of Leeds 8 of 10 So, this is an academic research project, and it performs-- the MUSE system performs named entity recognition and code reference on different text types and different genres. It uses this knowledge engineering approach with hand-crafted rules, and it works just as well as machine learning-based tools. And because it's hand-crafted one of the advantages is that they've been able to build rule-based systems for different modules separately. There's one to look at formatting. Is it a newspaper? Is it a letter or whatever? There's another for tokenization, just chopping it up into words. There's another for sentence-splitting, splitting it up into sentences, where the full stops and question marks are. Then you can have a part-of-speech tagger which labels the words as verb, adjective, noun, and separates out common nouns for proper nouns. Then you can have the gazetteer lookup which looks up each word in a dictionary to see if it's a named entity or not. Then you have these sorts of semantic grammars, for example, if you've got "entity earns entity" and the first entity as a person and the second entity is amount of money. You can also do coreference, so if you've got "Mr. Smith" somewhere and later on you've got "John" then both these names refer to the same entity. You can also have pronoun coreference. If you've got "John" as an entity and then later on you've got "he" then "he" probably refers to the same entity as "John" and the pronoun "he" can be resolved. The nice thing about this is you could adapt this fairly straightforwardly to other languages. First of all we did it for English because that's what they speak in Sheffield and then they developed it for other European languages like French and German and Romanian and Bulgarian and Russian. And then just as an intellectual exercise almost they tried it for some other languages just to see how difficult it is. Cebuano is a native language from South America. Hindi is a native language in India with millions of speakers or hundreds of millions of speakers. Chinese, Arabic, and so on, and because of these different components you could separate out the language-dependent components from the language-independent components. So, for example, coreference resolution works once you recognize what the entities are fairly well. So, for example, we mentioned Cebuano is not a language I know. It's used in South America by natives there, it uses the Latin or Roman alphabet the same as English or French or German, it has got capital letters denoting named-- proper names, sorry, names of things, so that works. On the other hand, there's very few resources for this language. In South America the official languages tend to be Spanish and Portuguese so most of the language resources are for Spanish and Portuguese and not for these lesser-known Indian native languages. That makes it medium difficulty. Hindi was another example they tried to develop for. This has a completely different alphabet, doesn't use the same script at all. So, you'd have to have different character encodings, it doesn't have capital letters, words do have spaces between them so you can work out what the words are at least. And because Hindi is one of the major languages of India and India is a big country with lots of computing research going on there are lots of resources available for it. That makes it medium difficulty for different reasons. OCOM5204M – Data mining and text analytics © University of Leeds 9 of 10 So, if you're going to adapt MUSE to other languages, what do you need? Well, you need to have a lot of support for other scripts, other character sets, and that could take up two-thirds of the actual time effort of development. And it's fairly-- if you like, it's not particularly AI research dealing with different character strings, character sets, but it is important and time-consuming. You also need to get hold of dictionaries for mapping. Ideally it-- very often you can get hold of an English to the other language dictionary because English is everywhere. So, for all-- if not for Cebuano you could get hold of a Spanish-to-Cebuano dictionary because-- or Portuguese-toCebuano dictionary. So, typically for most languages there is a language dictionary and a bilingual dictionary comparing that language to one of the major European languages. You also need to get hold of a corpus for evaluation. So, even though you don't need it for training machine learning you do have at least a small corpus where someone has manually annotated the names of the entities and the types of the entities and the relations so that you can check. You can then run your MUSE over this and come up with some results and then compare the results against the actual truth, what the human annotations were. So, you do still need to annotate at least a small test corpus for evaluation. You also have to be quite clever in looking for the internet to find resources. If you want to have a list of names of people, names of places, names of companies then maybe you can find these on the internet. There may be phone books or Yellow Pages or other resources on the web that you can mine. And then you can do text extraction from that to try to get the information you want. As I said, a big part of the problem is dealing with different character sets. So, they actually developed a GATE Unicode kit on top of just the core Java resources which allows you to type in and have virtual keyboards for lots of different languages and display lots of different languages. So, that's the input, and the output allows you to display, for example, this is Chinese and in green we have a location and in blue we have some persons and in purple we have the organization. I don't speak Chinese, but I have shown this to a Chinese speaker, and they have verified that they've got this right. So, this is just an example. To do proper evaluation, as we saw with Word2vec, the Mikolov research team, as well as the developing Word2vec they also developed a very large evaluation test set. So, to test or evaluate information extraction you can just show some examples. What we should really do is come up with a common, very large, standard data test set which other people can use, too. So, what's coming up in information extraction? Well, as I said, one very important thing is extraction evaluation sets. So, there's ongoing research into developing large corpora with information extraction labelled. You also need to develop tools for the semantic web, supposedly still coming, as you should be able to search for meaning on the web pages rather than just words on web pages. There's definitely a need for information extraction in bioinformatics and medicine. Particularly with COVID there's been a lot of funding for information extraction to try to find information about COVID, COVID sources, COVID cures, and so on. Another big area of information extraction is in finance industry. For example, to predict share prices you probably want to see what is happening in the world, and news can directly or indirectly affect costs of commodities, which then affect share prices. OCOM5204M – Data mining and text analytics © University of Leeds 10 of 10 You do want to be able to not just extract information, but answer questions, and to answer questions what Google has to do is find the web pages and then extract entities from that to answer the question. So, there's cross-fertilization between information extraction and information retrieval. Another thing apart from finding the entities is you need to find the relations between the entities. I haven't really said much about this in this lecture, but your Jurafsky and Martin textbook has a chapter on information extraction where they talk a lot more about the algorithms for machine learning, that type of information extraction. And also, the algorithms for relation extraction. So, read the chapter to find out more about this stuff. OK, so that's it for now. We've looked at information extraction, which is extracting named entities and relations from text rather than just information retrieval, which is finding the documents. Once you've found the documents the information extraction can find useful things in the documents. We looked at a few applications like the threat tracker or the kibbutz information system or the MUSE system. We saw that apart from machine learning, another way of building AI systems is by knowledge engineering, by knowledge engineers building a rule-based system. We saw rule-based named entity recognition is possible by coming up with rules at different levels, rules for named entities, rules for labelling the entities. And the MUSE system, the MUlti-Source Entity Recognition System, is one such example developed at Sheffield University by the Natural Language Processing Research Group there. And we finally looked at some areas for ongoing research and information extraction. And if you read the textbook in Jurafsky and Martin textbook chapter, you'll find more information about ongoing research, including machine learning research to do information extraction. OK, hopefully now you've got an overview. Please go and read the chapter in the book for a lot more detail of the algorithms and the ongoing research. Bye for now and I'll stop there. 
------------------------------------------------------------------------------------------------------------------------
OCOM5204M – Data mining and text analytics © University of Leeds 1 of 7 Cheat, NLTK, SpaCy: Text Analytics in Python Professor Eric Atwell: Hello, this is Eric Atwell, and, in this lecture, I want to talk about CHEAT and NLTK and other platforms for text analytics in Python. This is going to be an introduction to some Python tools, just an introduction. I'm not going into much detail. This too, specifically for computational linguistics or text analytics. And I want to look at one particular example, the CHEAT approach to a text analytic challenge, the Morpho Challenge. I'd like you to have a look at the website for NLTK and just go to nltk.org, and also to read a couple of short papers from conference proceedings on these topics-- one on the NLTK, Natural Language Toolkit by Steven Bird and another on the Combinatory Hybrid Elementary Analysis of Text or CHEAT system by myself, Eric Atwell. You could say a lot of computational linguistics or text analytics can be done by linguists and others, computer scientists even, without doing any programming. If you use Sketch Engine, you can collect a corpus or use an existing corpus, analyse it in various sorts of ways, get results out, analyse the data, and come up with conclusions. You can do text analytics experiments without any programming. And if it's not a functionality within Sketch Engine, then Weka is another toolkit, where you can load in data, run various filters to change its format in different ways, you can choose from a very wide range of classifiers, clustering algorithms, association algorithms, and so on. There's visualization tools, data analysis tools, and so on, all within Weka by graphical user interface pull down menus. And that works fine. If you want to try out some existing machine learning algorithms or other existing algorithms, but if you're a researcher or if you're doing a research project where you want to develop and test some new machine learning algorithm, it won't be in changing your Weka. By definition, It's new. So therefore, for those special cases, you do need to write some code. For example, in Python, you don't just have to write from scratch. You might look at some existing tools that are already there, like NLTK. I'm going to look in a minute at the CHEAT program. If you read the CHEAT paper there is a very simple Python program written from scratch, which does the analysis. But really, you shouldn't have to code from scratch using basic Python. For example, if you look at the Google Code Archive, or NLTK, or many other toolkits, there are quite a lot of existing bits of code which you can reuse, and a smart program can be just a few lines calling on some library functions. This may sound a bit strange for a computer scientist, but you don't have to program. In fact, you shouldn't program unless you actually have to. The first thing you should do when you have any experiment, or any task is to look at the task and see what's the easiest way of doing it. And it may be there is already some tool like Sketch Engine which will solve your problem, therefore you don't need OCOM5204M – Data mining and text analytics © University of Leeds 2 of 7 to code. So, don't code unless you really have to. If for example you're trying to design a search project proposal, you can think about what sorts of methods you're going to use, write up a work plan, and without doing any coding at all. Let's look at Natural Language Toolkit. It is only one example. If you google text analytics Python or computational linguistics Python tools, this will probably be one of the top ones you find. You may find reviews of different tools and NLTKs has been going on for a long time. And so, it's there. NLTK is the leading platform for building Python programs to work with human language data. Well, they would say it's a leading platform. This is a quote from the website, It provides an easy to use interface to over 50 corpora and lexical resources, such as WordNet. WordNet is a resource which has words grouped together into what they call synsets, groups of words which have similar meaning. And they have relationships between them as well. And you can call this WordNet network or ontology directly within NLTK and others as well. There's also a suite of text processing libraries for text classification, text tokenization, stemming-- that's working out that stemming is made of "stem" and "ing," or tagging-- like tagging is a verb in this case, as opposed to an adjective, parsing-- working out the grammatical structure of sentences, semantic reasoning-- working out the semantic structure and making inferences from text. There are also wrappers for industry strength NLP libraries, so you can access tools outside of NLTK from within NLTK wrappers. And there's also very importantly, an active discussion forum. If you sign up to the NLTK discussion forum, then if you have a question, then you can pose it and get answers from other people, or you can just listen in to see the other discussion, get good ideas for ways of doing things. It also has a hands-on guide, introducing programming fundamentals alongside topics in computational linguistics. There's an online guide with lots of API documentation so how to connect to NLTK from other tools or how to connect NLTK to your other programs. And there's a textbook. The authors Steven Bird and others have developed this over many years and written quite an extensive textbook on introduction to natural language processing using Python. I did consider using this textbook for this course, but it is a bit specialized. It is specifically about Python programming and there are other courses, other modules, in the program where you learn more about programming. I thought for this module, we'll think more about the theory and underlying methods and assume that you can do programming, so I don't want to spend too much time on programming. The NLTK also has lots of tools not just for text analysis, but for testing your classifiers, displaying results, things like that. For example, if you want to use the parseer to work out the grammatical structure of a sentence it will do that and it will come up with internal data representation-- the internal data structure that if you want to display it on screen, then you want to draw a tree structure. And here we have three lines or two and a half lines of text of Python code from NLTK.corpus import treebank. Treebank is a module containing lots of parseed corpora, where each sentence has a parse tree attached to it. And then you create a variable t, and you give it the value treebank parsee sentences from wsj_001.mrg. And WSJ-- if you know anything about computational linguistics-- that's the Wall Street Journal, one of the very first large scale corpus or corpora of American English from the Wall OCOM5204M – Data mining and text analytics © University of Leeds 3 of 7 Street Journal, the big newspaper in New York. And they've taken those texts at every sentence, added a parsee tree manually, so that we have a treebank or collection of sentences with trees attached. And this is just extracting the 0 for the first sentence in there with its parse tree. And t.draw, draw is just a way of drawing the parse tree. And we see here is a picture, there's actual sentences, Peter Vinkin, 61 years old, will join the board as a non-executive director November 29th, full stop. That's the sentence. And we see that the sentence structure is that first of all, there's a noun phrase subject, Peter Vinkin 61-year-old. And then there's a verb phrase, will the board as a non-executive director November 29th. And finally, there's a final punctuation, full stop, and so on. I won't go into more details but you can read this tree as a human, whereas the internal data structure won't be so readable. It will be usable within Python. NLTK is not the only toolkit available. There's plenty more out there. SpaCy is another one that's widely recommended. I'm not going to go into more details but you can click on this link. If you just google SpaCy or go to spacy.io you'll find out more. SpaCy is particularly popular because it has connections to some of the latest research systems in natural language processing. NLTK, because it's been developed over many years, it has lots of resources in it but it's more designed for educational purposes and has less direct connection to industrial interfaces, I guess they could say. If there's anything missing in NLTK, there's even a wanted list, so you can specify, I would like NLTK to have x, and then the development team will consider adding it to NLTK. Apart from general toolkits like NLTK and SpaCy, there are also toolkits for specific sorts of NLP, natural language processing tasks. For example, Gensim has very good techniques for topic modelling and for modelling semantics with sentences and words as vectors for things like, extracting the topic of a document. You could also, if you just google NLP toolkits or Python NLP toolkits, then you'll find Google will turn up numbers of surveys for example, The Top 10 Python NLP Libraries of 2020. That's the first of these links and another one is, Python Libraries for Natural Language Processing, just as a survey of them. And as well as just listing them, these two surveys also tell you what's a specialism of these things. For example, Gensim is in both of these lists and it's specifically good for topic modelling. And NLTK is in both of these lists and it's particularly good for learning and university education, it's saying. It's widely used in universities for teaching natural language processing. All these toolkits are very useful if you're going to be developing some new variant of an algorithm or if you want to try a range of different algorithms for some new tasks. So as I said before, if you're trying to build some experiment using known techniques, you might consider simply using Weka or Sketch Engine or some graphical user interface where you don't have to do any programming. You really only want to do programming if you're doing something novel which hasn't been done before, therefore isn't available in one of these tools. Talking about novel AI research, I have already recommended that you read conference proceedings papers from latest AI and computational linguistics conferences. Very often, these conferences have papers by researchers on the latest research they've just done. For example, the Banko and Brill OCOM5204M – Data mining and text analytics © University of Leeds 4 of 7 paper on scaling to very, very large corpora was a novel piece of research by Microsoft Research Labs and they thought they might as well publish it. Conferences often particularly in AI and machine learning and in text analytics or computation linguistics, they often have as well as these sort of one-off individual researcher’s conference papers or groups of researchers conference papers, they have what they call a shared task or maybe several shared tasks. And the idea is that some organizer has provided an annotated data set and some sort of evaluation and training data sets and a particular task, something that you have to do. You have to learn something and then anybody can participate. And each of the participants tries to solve the task, comes up with a score using the standard evaluation data set, and then they write a paper about their approach to solving the task and what their score was. And then at the shared task workshop, people can compare their methods and their results and there's usually some winner who gets a handshake or something like that. SemEval is a good example in text analytics. The magic evaluation tasks for SemEval conference every year has research papers by research groups and also maybe 5 to 10 different shared tasks or competitions. One that I was particularly involved in, not in SemEval but separate from SemEval, was one called Morpho Challenge. And there's the website for Morpho Challenge or if you just google Morpho Challenge you'll find it. And this was about unsupervised machine learning segmentation of words into morphemes. What that means is a segmentation is-- for example, the word unsupervised can be chopped up or segmented into "un," "supervise," and "duh." Or possibly, it's "un," "super," "vise" and "duh." And the task is to come up with an algorithm for taking text, not just English text though, in difficult languages like Finnish and Turkish-- Finnish and Turkish are notorious or famous for having quite long words and each word is made up of many morphemes. In English, most words are just one or two or maybe three morphemes. In Finnish or Turkish, they have longer words with several morphemes in them. The task has to be unsupervised. That means you just get given a list of words in Finnish, let's say, and your algorithm has to somehow work out just from a list of words and from repeated patterns in the words what are likely to be the morphemes. So for example in English, if there's a lot of words starting with "un," that's a clue that "un" is a morpheme. It doesn't tell you what "un" means, but at least it gives you a clue that "un" is a morpheme and whatever is left is also morpheme, so unsupervised suggests that "un" a morpheme and "supervise" is a morpheme. There's lots of words ending in d or e d, which suggests that "un" is a morpheme "supervis" or "vise" is a morpheme, and "ed" or "duh" is a morpheme. OK, so that's unsupervised machine learning for segmentation of words into morphemes. And because we were developing new machine learning algorithms, we couldn't just use a toolkit, so we had to code in a programming language like Python. And if you read the paper, you'll see there is an actual example of my first Python program which does this. It didn't work perfectly, so then I had to get my collaborator Andy Roberts from Pearson. Pearson is a company you might have heard of because they recruited you in the first place, maybe. Anyway, OCOM5204M – Data mining and text analytics © University of Leeds 5 of 7 Pearson allowed Andy to work with us on developing a rather more sophisticated version, essentially the same algorithm but with some extra nuts and bolts in it. The very first Morpho Challenge was back in 2005 and I thought this was a great idea. They had this task, had some data sets, that is lists of words in Finnish and Turkish and also in English, just for people who can't cope with difficult tasks like Finnish and Turkish. And the task was simply come up with a unsupervised machine learning algorithm, which will be able to segment words into their morphemes. We were given a training set and a separate test set, and we had to produce our results on the test set and send them off to Helsinki University in Finland, who organized the conference. And I thought this would be great. This would be quite challenging for MSc students to do, so let's do it as a piece of coursework for MSc students. Because it was quite hard, I said pairs of students can work together to come up with a solution. Some of the lead students took this very seriously and actually entered the real contest. But even those who didn't, I was able to get their results because they had to submit to me to mark their coursework and get a grade from me, their code and also their results. And that meant I was able to use their results in an ensemble. CHEAT is actually an ensemble classifier which combines the results from several different Leeds student entries into a combined system. And as we saw in for example, Banko and Brill found that an ensemble of several classifiers is generally better than one individual classifier. And the same thing happens with unsupervised learning. An ensemble is generally better than all of them. So now to end this lecture, I'm going to give you a treat. I'm going to show you a video from 2005 because we presented the paper at a conference and the conference was actually-- this was a novel, new idea at the time. Why don't we video record the lecture, the talk, and then put the video recording on the internet so our people can see it too? And this was because back in 2005, a company called YouTube started and they invited people to upload their own short videos of people doing fun things like dancing and smiling, or the very first one that got famous was somebody went to the zoo, and they took some video of elephants, and they said elephants are really cool because they've got really long trunks. And that was it. That was amazing. It's a bit like TikTok now. My granddaughter is into TikTok. TikTok lets you record something, and you have a maximum of, I think it's two minutes, for your video. Back then YouTube allowed 10 minutes. And if you click on this link, you'll see not the very first YouTube video, but rather a news story on, I think it's CNS news, some American news channel, talking about the start up company YouTube. That's interesting. Of course, academics at the time knew that we tended to give lectures that are longer than 10 minutes, so a group of academics set up another website called videolectures.net. And here we have, if you click on this, my lecture-- It's a bit longer than 10 minutes-- on the CHEAT ensemble. This is the presentation given at the conference. OCOM5204M – Data mining and text analytics © University of Leeds 6 of 7 The presentation was actually based on a PowerPoint slide where I had pre-recorded my voice on each slide and then for the lecture, I just played each of them one by one. That's what I'm going to do now. If you want, you can click on this and watch the video-on-video lectures. Unfortunately, the technology they used at the time may not work on some of our current browsers, so you may have difficulty actually watching the video there. What I've done is I'm now going to include the video, because it was recorded in 2005 on very early technology, sort of Black and white movie, not almost, not quite. It's quite a crackly sound. I say black and white, It's not black and white. It's green background. You may wonder why the backgrounds for these slides are different from the backgrounds on other slides. This is the standard University of Leeds PowerPoint format from 2005. Back in 2005, we had to have this University of Leeds logo here, and we had to have a green band here, and a green background, and use black text on green background. This whole lecture has been in the style of a 2005 lecture. And to finish off now, I'm going to go into the talk I gave on the CHEAT system. Enjoy and thank you very much for listening. And I'll hand over to Eric Atwell from 2005. [AUDIO PLAYBACK] This talk is entitled Combinatory Hybrid Elementary Analysis of Text, The CHEAT Approach to Morpho Challenge 2005, A Carefully Crafted Acronym by Eric Atwell and Andrew Roberts, with the help of Eric Atwell's Computational and Modelling MSc Class, Karim Ahmad, Adolfo Allendes Osorio, Louis Bonner, Sa Chaudhuri, Min Dang, David Howard, Simon Hughes, Iftikhar Hussain, Li Ki Ching, Nicholas Molaison, Edward Manley, Kalid Rehman, Ross Williamson, and Hong Tung Xiao. Our guiding principle is to get others to do the work, as inspired by Homer, Homer Simpson that is. We all know that plagiarism is bad but in software engineering reuse is good. However, we can't just copy results from another entrant. But, we may get away with some smart copying. We can copy results from many systems, then use these to vote on analysis of each word. But how can we get results from the other contestants? Well, we decided to set Morpho Challenge as an MSc coursework exercise. Students must submit their results to the lecturer for assessment before the contest. But you might say, is this really unsupervised learning? Well according to the Morpho Challenge website, the program cannot be given a training file containing example answers. Well, our program is given several candidate answer files but it doesn't know which one, if any of them is correct. So, it is unsupervised learning by these definitions. Moreover, it is inspired by the recent film Super-Size Me. We call this triple layer super-sized unsupervised learning. Firstly, unsupervised learning by the students, then unsupervised learning by the student programs, thirdly unsupervised learning by our cheat.py program. The first stage isn't really machine learning but unsupervised learning by students. Eric Atwell gave background lectures on machine learning and on the principles of morphological analysis. So, the students were not given example answers, in this case unsupervised morphology learning algorithms. OCOM5204M – Data mining and text analytics © University of Leeds 7 of 7 Students had to work this out for themselves, so student learning was effectively, unsupervised learning. The second layer was unsupervised learning by the student programs. Pairs of students developed Morpho Challenge entries. For example, Sa Chaudhuri and Min Dang and Khalid Rehman and Iftikhar Hussain both developed systems that are published in the proceedings. But as far as we were concerned the student programs were just black boxes. We just needed the results, the output of the programs. The third layer was unsupervised learning via Python program cheat.py. It read outputs of the other systems line by line and then it selected by majority vote analysis. If it's a tie, it selects the result from the best system, the one with the highest F measure. And then, the program just outputs this result, which is our result now. The CHEAT program was simple enough to be printed in full in the proceedings paper and it worked in theory. But unfortunately, some student programs read in the input and reordered the word list before putting it out again, so outputs were not aligned like we like. Luckily, I managed to get Andrew Roberts to develop a more robust cheat2.py and this program really did work. [END PLAYBACK] And here we have some examples of the outputs. Notice that black box is the combined results and the ones before it are this student in the wants for Finnish and the same thing for Turkish. The black box is the actual combined result, and the student ones are the other ones. Notice that at all one isn't the best by any means, but the conference organizers took this t approach and here we have the last two columns. They combined C orders, they combine all the results and gave a system which was quite good but not brilliant. But if you combine just the top five, then C top five is better than any of the existing systems. This is the example for Turkish. [AUDIO PLAYBACK] In conclusion, we find that machine learning and student learning can learn from each other. It turns out that our CHEAT program is actually a committee of unsupervised learners. One of our reviewers pointed this out. For example, see Bank-owned Brills 2001 paper. But we didn't actually learn this from literature until after we'd done the program, so maybe this is a forth layer in supersized unsupervised learning. But CHEAT is also a novel idea in student learning. The idea is to get students to implement the learning programs. And so, the students learn about machine learning as well as about the domain, in this case morphology. All in all, the Morpho challenge contest inspired our students to produce outstanding coursework, so it was worth it for them. We'd like to thank the Morpho challenge organizers for a truly inspiring contest and thanks to the audience for sitting through our presentation in absentia. Bye. [END PLAYBACK] 
--------------------------------------------------------------------------------------------------------------------------
OCOM5204M – Data mining and text analytics © University of Leeds 1 of 27 Chatbots and Dialogue Systems Professor Eric Atwell: Hello. This is Eric Atwell and this lecture I'm going to be talking about chatbots, also known as dialogue systems, and other things too. So, first of all, we're going to have an introduction to chatbots and dialogue systems and then have a look at properties of human conversation, since chatbots are supposed to mimic or emulate humans, and then look at a couple of ways of building these chatbots. One is rule based system. And we'll look at some examples, ELIZA, PARRY, the AIML, artificial intelligence markup language, and then a couple more recent chatbots, ALICE and Hubert. And then another way to do this is corpus-based chatbots, using a corpus to train via some sort of machine learning, and then look at the more complex dialogue state architecture for some current chatbot platforms. And to end up with, look at a couple of issues evaluating dialogue systems or chatbots and ethical issues with chatbots. OK, so let's start off by a general introduction. First of all, as with computational linguistics or text analytics or natural language processing, there are different names for this topic area. Conversational agents, also called dialogue systems or so-called dialogue agents. The general term seems to be chatbots. From different perspectives, an agent suggests that they are intelligently trying to do something or achieve some goal. Conversational sounds more like chat, whereas dialogue is more like interaction to some purpose. Chatbots, bots means a robot of some sort. And you're probably familiar or have at least seen personal assistants on phones or other computing devices, like SIRI for Apple, Alexa from Amazon, Cortana from Microsoft, Google Assistant... I don't think they've got a fancy name for Google Assistant. You just say, hey, Google. All of these seem to be chatbots. In addition, you can...to talking to them via typing, you can actually literally talk to them, and they will give you a verbal response, an oral response. OCOM5204M – Data mining and text analytics © University of Leeds 2 of 27 However, we don't really know what's going on behind the scenes. So, these are apparently very smart systems, but on the other hand, if you try hard enough it's easy enough to ask questions which they just don't seem to be able to answer properly. They are aimed at a very general audience. They're not for specific purposes, but they have got a number of what are called skills or things like that, which are things they can do. So, they're particularly good for playing music. I can say, Alexa, play me my favourite Sex Pistols song. Now try to guess which one it is. It can set timers. Alexa, tell me in an hour’s time. Clocks, various functions that they think people will need to do. It can also do chatting for fun. You can say, Alexa, tell me a joke. There are some more, if you like, serious things you can do...booking travel reservations, ordering, shopping via Amazon if you're using Alexa. They can also answer questions, at least general knowledge questions. And typically, Alexa, if it hasn't got...if it's not a very straight forward answer, it will say, here's something I found in Google or Wikipedia or whatever. And clinicians, hospitals, clinical researchers have started to use these for mental health issues. So, if you have mental health problems, one of the things that the patient has to do is talk to a clinician or a mental health practitioner, or a chatbot. And you can either just simply let it talk with the chatbot and hope that will improve things, or you can actually keep a transcript of what's being said and then afterwards analyse the transcript to do some sort of diagnosis. This does suggest there are two kinds of chatbots. There's your chatting conversational agents which mimic informal human chatting, generally just for fun or for relaxation or for giving an individual a sense that they've got someone with them. And this potentially has therapeutic effects if you have mental health problems or are feeling lonely. But there are also the more formal targeted task-based dialogue agents, which have a particular task in mind like booking flights or booking restaurants or buying things via Amazon. There are also interfaces to personal assistants. So, it would be very nice if every student, every academic had a personal assistant to deal with things like when is my next lecture or when is the OCOM5204M – Data mining and text analytics © University of Leeds 3 of 27 coursework due, and there are things to help with that sort of thing. Also, when driving a car, turn on the air conditioning or giving robot instructions or even given washing machines instructions. Another sort of area for chatbots is question answering, ask a question. It's not clear if this counts as a conversational agent or a task-based agent, but there are certainly...for Amazon I can ask questions and sometimes it's more like human conversation. It's not particularly fact-finding questions. But if I'm trying to do some coursework, maybe I do need very task-oriented questions. OK, so that's two types of chatbots. There are also, in general, two types of chatbot architectures and this also mimics the general sort of two types of architectures for AI systems and natural language processing systems in general. You can either have rule-based systems where the computational linguist writes some rules, tries it out, and extends the rules until eventually the system works reasonably well. Or you can have machine learning from a corpus where you have a training corpus appropriately annotated and the machine learning algorithm extracts, essentially, rules like the ones that the human did in the first version. So, very simple rule-based systems have patterns and actions. So, there's a pattern and then the user input is matched against this set of patterns until the best pattern is found and then the corresponding action with that pattern is done. And the pattern can often be a question type and the action will be an answer, so that fits very well for question answering systems. In addition to that, you can stick on top of that some sort of infrastructure. As we'll see in a minute, PARRY has a mental model, it’s responses aren't just based on the input patterns, but also its state of mind. And this actually produced transcripts which psychologists examined, and they weren't able to tell if PARRY was actually a human or a chatbot and, in some sense, it passed the Turing test. The Turing test is can human judges decide if they're conversing with an AI agent or a real person, and the psychologists weren't sure. Corpus based system. Well, as we saw before, there's two sorts of general approaches. One is an information retrieval approach, encoding the input into some sort of vector and then finding in the OCOM5204M – Data mining and text analytics © University of Leeds 4 of 27 corpus the closest, the most similar vector using information retrieval techniques, and then outputting that. And the alternative nowadays is to use deep learning neural models to find the best match. OK, so here's some examples into action. We've learned the bot a neural network system. "Will you sing me a song? Sure. What do you want me to sing to? I can sing a song about baking”. And this looks like a plausible sort of interaction. Notice that the BlenderBot doesn't actually sing, so then the bot can't sing. It can simply, given the input, find an appropriate match in the training corpus and output that and it happens that in the training corpus there was probably some interactions where there were baking and singing going on. That's why it decided to volunteer to sing a song about baking. Notice also that from the transcript, it's quite hard to see what the underlying architecture was, and this is certainly true for SIRI and Cortana and Alexa. You can try things out and these companies rely on the fact that people on the whole aren't that inquisitive about how the underlying algorithm works. They just want it to apparently work. And here's this Chinese system developed by Microsoft China. I think it's called Xiaoice. Something like that. I'm afraid I can't pronounce this. Watch the video and you'll hear how it's pronounced. OK. And again, you can see this is a chatbot very widely used. And there's actually a paper in The Computational Linguistics Journal, The Journal of the Association for Competition Linguistics describing in great detail more about the underlying algorithm, and it's for chatting to ordinary people in Chinese. When you start the app, you are advised you are not you're chatting to an AI agent, not a real person, but quite a few users go on for quite long conversations and think perhaps it is a person. So, that's a couple of examples for task-based dialogue agents. They're not just simply chatting, so we don't necessarily have to have a huge training corpus covering all things that someone might say. But it is usually first focused on a task like setting a timer or making a travel reservation, so you have a model, potentially quite a small model, on a very specific topic. OCOM5204M – Data mining and text analytics © University of Leeds 5 of 27 And of course, what Alexa does, for example, is you have skills. Each of these skills is very much based on a goal or task, but you can basically power up as many skills as you want or have available several skills at the same time. So, that's how you do it. You basically have components or widgets for each of the skills and the more and more of these have, the more generic or general system has. So, the architecture for a particular skill is to basically have a frame with slots and values, a sort of knowledge structure representing user intentions rather than just recording words and phrases. So, the frame may be a set of slots that the system tries to build in or fill, and the assumption is that in the interaction the user wants to fill the unknown slots and the system can help in filling the unknown slots. Each is associated with a question to the user. So, for example, if you're trying to build a travel reservation, then the system knows that it has to have an origin, a destination, departure time, a departure date, and an airline if it's an airline booking system. So, in making the reservation, the system has to fill these slots and the user also knows probably that these are the bits of information you have to be given, so they're typical questions that the system can ask to try to fill each of these slots. OK, so that's the general overview of what sorts of chatbots there are. There are conversational chatbots or task-based systems, and the architectures are either rule-based systems or corpus trained machine learning systems and they're either general conversations or they're focused on particular tasks. Now all of this, for a human user, they expect the conversation to be human-like. So, this is a big problem that if you have a graphical user interface, then the graphical user interface designer can pretty much decide whatever they want to put on it, knowing that the human will realise that this is a graphical user interface. It's not a real person. But when you start conversing with a chatbot, you start making assumptions that it's going to talk in a natural human way. Here's a typical telephone conversation between a human travel agent A and a human client C. And we'll see it's not just question, answer, question, answer, question, answer, but there are all sorts of strange extra things happening. OK, I'm going to point out a few of the examples. You don't OCOM5204M – Data mining and text analytics © University of Leeds 6 of 27 actually have to memorise this to pass the test at the end. OK. You see, in general there are these turns, but they're not simply question followed by answer followed by question followed by answer. It is a sort of game where two people take turns. In fact, in a real conversation you may have more than two people in and they all take turns. A turn typically is a sentence, but it can just be a single word like OK, or it can be quite a longer sentence. Here's OK and you can respond OK just to say that it's my turn so I'm going to say something, but I'm just going to agree with you and that's it. Now you can take over your turn again. Well, sometimes, this turn can be quite long. The client says, "what are they?" And the agent has to give quite a few possible options. And if it's turn taking, you have to then know when it's your turn...when do you take the floor? Or when is it the other person's turn, when do you yield the floor? So, there's typically unspoken conversation. There's pauses or there is something in the intonation, the way the voice goes up or down which we learned to interpret in this way. You can also have interruptions. Notice agent 16 and client 17. The client starts talking before the agent is finished. He interrupts. And the human agent knows the interruption is coming, so he should stop talking. And he also knows that the client is about to make a correction. So, the system has to be able to do this too. Now the agent says OK, there's two non-stops and at the same time...that hash means that they're happening simultaneously...at the same time, the agent's saying two non-stops. The client is saying, actually, and he's going to say, what day of the week is the 15th? So, he's going to change his date from the 15th possibly. So, in human interactions, you are allowed to barge in. You want to allow the user to interrupt. And there's also this end-pointing task for a speech system of deciding whether the user has stopped talking. You can't really just wait for a long pause otherwise you have lots of long pauses in the interrupt. People do also pause in the middle of turns while they're thinking of what to say. So, you can sort of model the interaction as a series of turns, and each turn is a kind of action. OCOM5204M – Data mining and text analytics © University of Leeds 7 of 27 So, philosophers and linguists like Wittgenstein also have thought of it as being a dialogue act or a speech act, and there are types of these speech acts or dialogue acts as constatives committing the speaker to agreeing to something. OK? I concur with you. That's not actually introducing any new information other than I am agreeing with you. Or as a directive, trying to get someone to do something. Attempts by the speaker to get the addressee to do something. I ask you; I'm asking you or forbidding you. There's also commissives, committing the speaker to do something themselves. I promise to do this. They can also be acknowledgments, expressing the speaker's attitude regarding the hearer. I am sorry. Hello. I thank you. So, these are different types of acts. And if you go through a dialogue corpus of spoken dialogue, you will see there's plenty of examples and you can try annotating each of these things. This is a bit like part of speech tagging or named entity tagging. In this case, a whole piece of speech is labelled or tagged as being a directive or a constative or acknowledgment. And in a chatbot interaction, the same sort of thing should be true. You shouldn't simply have question followed by answer, question followed by answer, but you have to have these things in too like OK, which is an acknowledgment. There's also the grounding principle. Participants in a conversation need to have a common ground. The principle of closure. Agents performing an action require evidence, sufficient for current purposes, that they have succeeded in performing it. So, if you're having a speech act, then each of the speakers need to be sure that they understand what's going on so far and they acknowledge that the hearer has understood. That's why you have to say, OK, every now and again, or some sort of sound, like uh-huh to say, yes, I understand what you said so far and continue. You might think this is just in speech, but actually, this is a common thing in human computer interfaces. For example, you may have noticed in an elevator when you press the button it lights up. And you might think, why do that? OCOM5204M – Data mining and text analytics © University of Leeds 8 of 27 Well, it's to show, to acknowledge that the elevator does know that it has been called. If you press the button and nothing happens, then you're likely to keep pressing the button. But when light comes on, you know to stop pressing the button. And it's just that this is like an, OK, I understand action. So, here we have an example. "You said, returning on May 15? Uh, yeah, at the end of the day. OK”. You might have, "OK. I will take the 5ish flight on the night before on the 11th. On the 11th? OK”. So, you can have just OK, or you can even repeat something...repeat the part of the utterance that you are confirming, that the 11th is the important bit in what he just said. And "I need to travel in May. And, what day in May did you want to travel?" And means I acknowledge that you need to travel in May but in addition to that, I need to have some further specification. So, conversations have this structure. And they have, as I said, turn taking, or you might say adjacency pairs. So, question and answer is the most common pair of things, but you can also have a proposal followed by an acceptance or rejection or a compliment followed by a downplay. Nice jacket. You might have an acceptance...thank you...or you might have a downplay...oh, this old thing. So, there are pairs of interactions and furthermore, these overlap. So, the question may be followed by an answer and the answer is then followed by another question. A bit like bigram models or Markov models, it can also have sub-dialogues. "OK. There's two stops. Actually, what day of the week is the 15th? It's a Friday. Hmm, I would consider staying an extra day till Sunday. OK. OK. On Sunday I have--" So, this is a correction sub-dialogue. The extra bit changes the day and therefore, you go back to where you were before only with the day parameter changed. Another sort of sub-dialogue is a clarification. "What do you have going to blah, blah, blah on the 5th? Let's see, going where on the 5th? Going to Hong Kong. Oh, here are some flights”. So, this clarification, the system didn't understand where they were going, therefore asked for clarification and then returned to what do you have going to Hong Kong in this case. You can also have some presequences. OCOM5204M – Data mining and text analytics © University of Leeds 9 of 27 The user actually wants to reserve a seat on the train to New York, but first of all he asked, "can you make train reservations?" And the system said, "yes, I can”. So, this is before you actually ask the question, you're asking, is this a reasonable question to ask? So, there's also some extra complications to human interactions. Conversations are not typically just one person in charge, and the recipient is answering the questions. There are special cases like reporters asking a chef or reporters interviewing someone. But typically, most human conversations have mixed initiative. I lead, then you lead, then I lead. So, natural language processing systems are quite difficult to do this because they have to allow for the human to take charge and then for the system to take charge and so on. So, often they simply have either the user initiative system where the assumption is the user's going to ask a question, the system responds. So, for example, IT help desk. The assumption is the IT user is going to want some information, so all we have to do is answer the user's questions. Or the alternative, of course, is to have a system issue. The system is asking questions, for example, to fill out a form. So, Hubert chatbot is trying to get students to give some feedback on the course and ask specific questions like, how could the course be improved, what did you think of the lecturer, and so on. The users can't really to take over and start asking questions back. Both of these are not really conversations, they're either just answering the user's questions or just asking the user questions. A further complication which we won't even go into here is the system has to try to make inferences, like the agent says, "and what day in May did you want to travel? The client says, "OK, I need to be there for a meeting that's from the 12th to the 15th”. And from that, you have to infer that he has to travel before the 12th. It's no good traveling on the 12th because he won't be there for the meeting on the 12th. So, that's an inference the system has to make. The client hasn't explicitly said I need to travel before the 12th. And some of the challenges of human OCOM5204M – Data mining and text analytics © University of Leeds 10 of 27 conversations, in reality, human chatbots...human interactions are more complicated than chatbots. Chatbots can't really do this. OK, so now let's have a look at some examples of real chatbots. So, ELIZA. You've probably heard of very famous first chatbot Weizenbaum back in 1960s developed this. It's an example of a transcript. Weizenbaum was a psychologist. He developed a chatbot which behaves in the style of a psychotherapist, and it's a particular sort of psychologist, a Rogerian psychologist. And the Rogerian approach is to engage the client in conversation and aim to get the client to understand their own problems by talking about their problems themselves. The idea is to draw the patient out by reflecting the patient statements back at them, and this is good. This is good as a chatbot technique because you don't actually have to know anything about the client. You assume the pose of knowing that almost nothing about the real world. Whenever the client says anything, you just invite them to say more about it and then we don't have to know about what they're talking about. For example, the patient says, "I went for a long boat ride”. The psychologist wants to get the patient to talk more about it, so you say, "tell me more about boats”. You don't assume what the boat...don't know what a boat is, you assume that the psychologist has some sort of goal, which is to get the patient to talk about things. So, there is a contest every year or so organised by Hugh Loebner called the Loebner prize contest, which is sort of a version of the Turing Test where several chatbots can enter and they're evaluated by humans to see if they think that they're human or not. And the ones that win or the ones that try to win usually choose some sort of domain like this where they pretend they don't understand much about the world and they get the user to talk about it. So, how does this work? So, ELIZA is based on a large set of rules that Weizenbaum devised, and the rules are essentially some sort of pattern. And then the user input is matched against these patterns until it finds one pattern that it decides to use. And then if the pattern is matched, then the transformation rule which generates an output. OCOM5204M – Data mining and text analytics © University of Leeds 11 of 27 So, for example, zero you, zero me. Zero means any text. So, anything followed by you followed by anything followed by me is a pattern, and then the output or transform me is what makes you think I something you. The three in this case is the third word or the third constituent. So, do you like me. Like is the third one. What makes you think I like you is the response. "You hate me. What makes you think I hate you”. So, rules are organised by these keywords. Each keyword has a pattern and a list of possible transforms. So, the key word might be “you” and the pattern involving you is something, you, something, me and the possible responses, not just one but several of them, and then the ELIZA platform can choose one of those. And there's a sort of general structure for each keyword. There may be a pattern followed by several transforms and there may be several patterns, each of which is followed by several transforms. OK, so that's a general set of lists of patterns. Then these keywords are then in order or ranked from a very specific one to a very general one. So, “I” is very general, so that would be down near the end. And the reason being that if the input doesn't match a very specific keyword, then it should match at least some of these things. So, “I star” means any sentence starting with “I”, you can come back with you say you something or other. "I know everybody laughed at me. You say you know everybody laughed at you”. “Everybody” is more specific so you can have a more specific...if everybody followed by anything at all, then you can ask, who in particular are you thinking of? So, key words are stored with their rank. “Everybody” has a rank of five and “I” has a rank of zero. And there's also a default set of responses. “Please go on”, “that's very interesting”, “I see”, and so on. There's also this idea, as well as the patterns, of a memory. Whenever “my” is the highest keyword, then you would select a transform on the memory list and apply it to the sentence. So, let's discuss further why your three and three is something on the memory list. So, it's possible...it's not just simply turn taking turn...we looked before as if every ELIZA reply was based on OCOM5204M – Data mining and text analytics © University of Leeds 12 of 27 what the user had just input, but you can also store away what the user had input earlier on in the memory so then you could go back to it. This is another human-like thing that rather than just simply having a Markov or bigram model, you can, at various interesting points, maybe even at random, go back to an earlier thing. Earlier you said that, or does that have anything to do with what you said earlier on? So, if no key word matches a sentence...if basically the user inputs something where you can't find a rule that does...you can either try one of these very general things like tell me more, or you can go back to the memory phrase. Then we see very simple rule-based architecture, and you can build your own chat box like this. I'll show you in a minute a platform you can try this out on. However, this a nice example where it's possible to implement an AI system and ELIZA it just went ahead and implemented it and tried it out without realizing some of the ethical implications. People became...at least some of them started to become emotionally involved with the program. One of Weizenbaum's staff was using it and asked him to leave the room because she didn't want him listening. She thought this is personal and private. He thought he was just going to store all the ELIZA conversations for analysis, being a psychologist, and it would be interesting to explore. But then people immediately pointed out, this is not ethical. There are privacy implications. You can't store people's conversations without their permissions. And this is a problem because you have to get their permissions in advance. You can't wait till afterwards because they thought they were having private conversations, even though they knew it was just a program. I suppose most ordinary people don't really have a sense of how the AI is not really intelligent, it's just a set of rules. That's ELIZA. If you want to find out more, look in the textbook or even...yeah, just Google ELIZA. So, one more example of an extension to this. So, we've seen a very simple model patterns, for each pattern some responses, and there's some random element in choosing the responses. And if there's no pattern that matches, you have a memory so you can introduce things that we've talked about earlier on. OCOM5204M – Data mining and text analytics © University of Leeds 13 of 27 So, that brings in the idea of having, as well as the patterns, some further overall architecture, and that's the idea behind PARRY. This is also a clinical psychology focus, but this is used to study schizophrenia. This is a model of schizophrenia because in addition to the ELIZA sort of structure, it also has a mental state model. It has these global variables for anger, fear, and mistrust. These are internally just numbers, and they all start with low variable numbers. And then after each turn, the user statement is analysed to see if it induces fear or anger. For example, if a user says an insult or if PARRY decides what the user said was an insult, that increases the anger. Or if a user says something nice, that decreases the anger and so on. But if the PARRY analysis says this is just an ordinary sentence, then the anger, fear, and mistrust can go down a bit. So, PARRY's responses depend on the mental state, or rather the value of the variable, fear or anger. So, if fear was high, then you go for one sort of responses which involve running away. While if anger is high, then the responses are more hostile and so on. So, you can see you can have essentially variables in the chatbot which is separate from the patterns and actions and these variables have additional features. And PARRY, in some sense, passed the Turing test in the 1970s. The Turing test is essentially a human has to judge or maybe some human judges if the output of the system is a chatbot or a real human. And in this case, psychiatrists were given transcripts of interviews with PARRY and interviews with ordinary people having paranoid schizophrenia and they weren't sure which was which. So, that did show that the PARRY transcripts looked reasonable. OK. So, if you want to have a go at building a system like ELIZA, there is a...Pandora Bots is one example website where you can build your own or try some that are there. This is based on AIML, the artificial intelligence markup language. So, AIML is a bit like XML or HTML, that's the markup language for web pages, but it allows you to write the sort of patterns that we saw in ELIZA. OCOM5204M – Data mining and text analytics © University of Leeds 14 of 27 Richard Wallace, an AI researcher, developed a version of ELIZA he called ALICE and put it online so that anybody around the world could interact with it. And in addition, what he did...ALICE learned, but not in machine learning, but in rather Wallace learning. In other words, he monitored the interactions and if ALICE gave a poor or implausible response, Wallace added some extra rules to fix this so that next time around if a user asked the same thing it would have a more plausible response. So, he essentially built up a huge bank of patterns and responses over time. ALICE built a very large pattern list of plausible replies. Now I mentioned the Loebner prize competition held every year where any number of different chatbots are entered and a number of humans, usually AI professors and other academics who know a bit about chatbots, they sit and interact with various systems and some of the systems are linked to chatbots. And some of the systems are linked to a person in another room and they're trying to judge which ones are human and which ones are not. And ALICE won the prize not just once, but three times for being the most plausible, largely because it was more plausible because it had more human interaction built into it. OK, so that's ALICE. However, you don't necessarily have to have a huge set of rules if you're for a very limited domain. So, for some practical applications...and there are commercial chatbots for limited domains because that's all you need to talk about. For example, in the book by...the paper by Abu Shawar I recommend you read on "Chatbots, Are They Really Useful?" she gives several examples, one of which is FAQchat. That is a frequently asked questions, and FAQchat can be trained with any set of frequently asked questions. There's a frequently asked questions for Python website and you can feed this into the FAQchat chatbot and then if you ask any questions, it will try to find the question in the FAQ which is most similar and give you the answer from the FAQ, so that's fairly straightforward. And if you ask, who is the president of the United States, then it will find the Python question most close to that and give you the answer. It will be the wrong answer, but then you shouldn't have asked OCOM5204M – Data mining and text analytics © University of Leeds 15 of 27 a silly question. OK? So, within the limited application of asking questions about Python, that is a quite sensible approach. OK, another example is when a chatbot is asking questions in a very specific type of domain. Remember I said that in real conversations, both partners take turns in leading the conversation. For chatbots, the simplifying assumption is either the user is asking questions and chatbot has to answer them, as in FAQchat, or the system is asking questions and the user simply has to answer them. And this is, for example, in student feedback interviews, Hubert asks, did you like course, what could you improve in the course, and so on. Or in job applicant interviews, the interviewer is in charge and the applicant simply has to answer the questions. That's what Hubert does. And there's a recommended paper on University student surveys using chatbots about our project here at Leeds University on using Hubert. OK, so there's some examples of chatbot systems that are rule based. Now going to see a couple of corpus-based chatbots and corpus-based chatbots are particularly good for general conversations. So, if you have a very specific domain, then you can build a set of rules from an FAQ website or from some other standard source. You go on a very general system, then you may have a big corpus and whatever you do as a question or input is, it has to match. It has to search the corpus for something similar and that generates the response. You can do this by using information retrieval to grab a response from the corpus or possibly a bit more sophisticated is use the corpus to train a language model, maybe some sort of encoder-decoder type neural network so that when the user typed something in, the trained language model, which is essentially trained on the corpus, will generate a response. And so, the first one is memory based learning or instant space learning. So, the retrieval says whatever the user input is, you search through the corpus to find the nearest match. The alternative is you use the corpus in advance to train a very large neural network, so the neural network essentially OCOM5204M – Data mining and text analytics © University of Leeds 16 of 27 encodes all of the corpus so that when the user types an input, the generated output is appropriate for the corpus. So, where did you get these corpuses from? Well, only computational linguists, also corpus linguists such as myself. We had collected corpora of language, which include not just written text but also spoken. So, the British National Corpus, 100 million words of British English, includes 10 million words of spoken texts transcribed and 19 million words of written text in computer readable form. And also, the international Corpus of English or ICE, that has for British, American, Australian, New Zealand, Canadian, and various other variants of English, a million words including a significant portion of spoken dialect transcribed, and it's not just English. So, for example, the paper on "Chatbots, Are They Really Useful?" mentions the Afrikaans chatbot, which was trained on the Korpus Gesproke Afrikaans, a corpus collection collected at Potchefstroom University in South Africa of Afrikaners speaking to each other. And it then, having trained on that, it was able to converse in the style of the Afrikaners. OK. Another way of doing this, quite popular in America, has been to pay people to talk to each other on the phone or even to give them free phone calls. So, transcripts of telephone conversations between volunteers, so the switchboard or call home corpora. So, call home means we will give you a free one-hour call to your parents or family back in wherever home is from America. So, you get lots of transcripts of, for example, Arabic, where Arabic speakers in America call home to their family in Saudi Arabia or Tunisia or wherever it is. And apart from that, there's lots of questions and answers data sets like the frequently asked questions or at Leeds University, the IT help desk has a very large data set of typical IT user questions and their answers, and this can be used to train a chatbot. OCOM5204M – Data mining and text analytics © University of Leeds 17 of 27 OK, there are other sources. So, there's a huge corpus of movie dialogues because there's lots of corpora of movie subtitles. Most movies nowadays, apart from the actual speech, they have subtitles added and you can extract the subtitles and that gives you the transcript of what was said. You can, if you want to, just simply hire people to have conversations. That's a bit like the call home thing, but it's usually for if you want to have a corpus on a very specific topic. So, if you want to have conversations about, let's say, how to get into university, then one way is to hire some people to talk about how to get into university. There are also pseudo conversations. If you go onto social media, you can look at interaction on Twitter where people respond to other ones or Reddit or Chinese Weibo and so on. These are noisy in the sense they're not really conversations, so they can be used as a sort of pretraining to get some sort of language model for the neural network type approach. One ethical issue for all of this is you must remove personally identifiable information. People will just not...obviously people's own names may not be mentioned, but they may mention other people's names or even their telephone numbers or their credit card numbers or all sorts of things that anchor telephone conversations, so you do have to be careful about that. OK, so how does this work? If you've got a large corpus, if a user says a Q, question, and it has got training corpus, then you find in the corpus the turn that is most similar to the question using something like tf-isf cosine measure and then you respond with R. Or perhaps a bit more complicatedly, having found something similar to the user question you say the response to R, assuming that the corpus is essentially a series of questions and answers. So, the user asks a question, you find the question in the corpus or something similar to it, the most similar to it, and then whatever the answer was, you give that. And that's a very simple model and it seems to work. OCOM5204M – Data mining and text analytics © University of Leeds 18 of 27 The neural method isn't that much more complicated. You give a user a question...given a user question, you find the turn in the corpus which is most similar. The difference is here you use BERT rather than using cosine similarity. BURT, we'll see later on what that is. That's basically a huge neural network for noticing if two sentences or similar or not. And then you just say response, or you say the response to the question. One final way you could use a neural network for is if you have a whole of a neural network encoding the corpus. In other words, you train the corpus for every sentence, every question and response. You put this into the neural network, so the neural network essentially builds a model of, for any input, what is the appropriate output. Then when the user typed in a question, a query, the response is generated by conditioning the encoding of the query and the response so far. So, you basically generate a response using the neural network. It's, as far as I can see, just a fancy way of doing what we've already done but using a neural network instead. And just as an aside that there is a slight problem with generating something which replies to whatever was said before because you can get into loops, if you like. If anybody here who...if you've heard of PG Wodehouse wrote a series of books about Jeeves very early on, about a hundred years ago. Here's a quote. "'What ho!' I said”. 'What ho!' said Monty. 'What ho! What ho!' 'What ho! What ho! What ho!' After that it seemed rather difficult to go on with the conversation”. So, what ho is a standard response to anything but if you then reply with “what ho”, then you're stuck. And here's some real examples. "Where are you going?” “I'm going to the restroom”. “See you later”. “See you later”. Don't say see you later in response to see you later because if you do, you're stuck. OK, so that's a slight problem. One extra thing apart from having a corpus is that to generate responses, you can also use a corpus to find informative text rather than simply a response to a dialogue. OCOM5204M – Data mining and text analytics © University of Leeds 19 of 27 So, Xiaoice, if you ask, tell me something about Beijing, it won't simply look in its corpus to find what word sounds like tell me something about Beijing and then respond with this next question...the next sentence. It actually looks into news articles or public lectures or things like Wikipedia and finds some information. So, that's another use of a corpus. OK. So, finally, chatbots are fun and they're good for narrow scriptable applications, things like answering questions about the IT help desk. On the other hand, you have to remember that they're not really people. They don't really understand, and the problem is they do give the appearance of understanding, so that may fool some people that may get involved with them. And they break down. The rule-based chatbots in particular can take a lot of learning lots of rules and they still, unexpectedly, don't give really silly answers. The information retrieval-based chatbots are as good as their training data. So, if there's something bad in the training data, they can give bad responses. So, maybe you want to integrate this chatbot into some sort of knowledge-based frame-based agents, and that's what the idea of a dialogue state or belief state architecture is. So, a more sophisticated has to have dialogue acts, and most modern systems don't just have a corpus trained chatbot or a set of frames, but some sort of combination. And this is working into these industrial systems. Things like Amazon Alexa probably have this sort of slot understanding as well as the chatbot understanding. So, here we have an architecture diagram. A typical user says something, there's automatic speech recognition, and then spoken language understanding which feeds whatever is said into a dialogue state tracker. And the dialogue state has a corpus of what sorts of things are likely to be responses, but also a state model of frames, like you want to go from downtown to airport and so on. It may have some sort of dialogue policy model, which says what's an appropriate thing to say back, and then it generates some natural language response as a text and then the text replies by text to speech. That's a more sophisticated model overall. It's natural language understanding which extracts it. It doesn't just do transcription of speech to text, but also extracts the slot fillers from the speaker. And OCOM5204M – Data mining and text analytics © University of Leeds 20 of 27 there's a dialogue state tracker which maintains the current state of the dialogue and some sort of dialogue policy where the system decides what to do next rather than simply replying with a best match. And the natural language generation module produces more natural utterances rather than simply whatever is in the corpus. So, each of these parts is a bit more sophisticated. So, here's an example. User says, "I'm looking for a cheaper restaurant”. And the system works out the important thing is cheap. The price has to be cheap. So, the system says, "sure, what kind and where?" And the user replies, "Thai food, somewhere downtown”. That means the system now knows is looking for cheap, he's looking for Thai food, and he's looking for downtown or centre of the city. So, the system finds something which matches and gives that as an answer. "The House serves cheap Thai food”. The user comes back with, "where is it?" That means the system now needs to know that he, as well as wanting cheap Thai...price cheap, food Thai, and area centre, he is also asking for an address, therefore comes back with an answer which matches all of that. Not just any old address, but an address which matches the previous answer, The House. OK. So, we see this more sophisticated dialogue tracking model. OK. Finally, I want to look at how to evaluate dialogue systems and some ethical issues in dialogue systems. So, evaluating is quite hard. You used to, for classifiers, coming up with an accuracy score or maybe a precision and a recall score. For dialogue systems you can't really say, is the response correct or not correct 100% in a conversation. If I ask you something, there isn't only one correct answer and everything else is just wrong. There are usually several plausible answers, several variations of answers. So, if it's a task-based dialog like trying to book a ticket, then you can measure the task performance. At the end, have they managed to book a ticket? If it's a conversational chatbot, am I having a reasonable chat with you? And this is down to human evaluation. You have to say, overall, did I like the conversation? Did I feel I was having a reasonable chat? And that's much more subjective. But in the whole, chatbots are evaluated by humans. The OCOM5204M – Data mining and text analytics © University of Leeds 21 of 27 humans talk to a chatbot and either decide, in the Turing test, was it human or not, or maybe give them a score on a rank. You also have observer evaluation, a third party who watches the conversation or maybe reads a transcript of a conversation and assigns score. As in the example of PARRY, PARRY had a conversation with users and then the psychologists were asked to rate whether or not that PARRY was having a conversation or a real psychopath or psychotic person...sorry...was having a conversation. So, in both cases is it it's by human evaluation. In the first case, it's the human user is getting to feel how good their talk was and the other one is to have a bit more objective, a third party. Maybe a conversation expert and AI professor could be doing this. And you can do this in even more sophisticatedly. Rather than just saying was it good or not, you can have a sort of very specified experiment. For example, human will chat with models for six turns and then rate on eight dimensions of quality. Did they avoid repetition? Was it interesting? Did it make sense? Was it fluent? Was the system listening to me? Was it inquisitive? Was it really human-like? Was it engaging? All these sorts of issues. And each of these, in the evaluation, particularly for having an objective third person doing the evaluation, they may have quite detailed definitions. So, rather than just avoiding were they repetitive, they have criteria like repeating themselves over and over or sometimes said the same thing twice or always says something new. So, that's three different levels of repetitiveness. So, for each of these different dimensions or features, you rate on a scale of one to three or one to five. So, you see that this can get quite sophisticated, and you end up with a score...not just is it good or not, but a more nuanced score for each chatbot. So, typically and the most objective way of doing it, the most scientific way of doing it, is to have people have conversations with chatbots and then have annotators...for example, AI professors or students...analyse the responses, look at the conversations, and typically to make it easier, you have OCOM5204M – Data mining and text analytics © University of Leeds 22 of 27 two conversations and decide which one is better. So, you might have a conversation with a real human and a conversation with a chatbot and decide which one is which. That's one way of doing it. And according to all of these criteria, engagingness, which would you prefer to talk to in a long conversation? It's a bit subjective, but you can see these are the sorts of things which you decide...it's not just is this a real person, but this is a natural person I'd like to be able to talk to? Automatic evaluation is still difficult. We saw from machine translation...machine translation is still pretty much judged by translators deciding this good or not. There are these blurred scores for machine translation, but there's no real equivalent for chatbot evaluation. There is a research direction called adversarial evaluation if you train a Turing-like classifier to distinguish between human responses and machine responses. So, you can actually build a machine learning classifier trained on examples of chatbots being positive...sorry…examples of chat box as against examples of human conversations and then given a new unknown conversation, it tries to decide whether or not it is chatbot or human. And the more successful a dialogue system is at fooling this classifier, the better of a system. It's actually quite hard because we haven't got lots and lots of training data and we're not quite sure it works that way, but you can imagine that's how it might work. The other thing is, as in any IT system, if you want to evaluate it you can have a user satisfaction survey. So, you can set something like Hubert to ask the user after a chatbot had...after they've had a conversation you can ask, for example, was the system easy to understand? Did the system understand what you said? So, you're actually asking in the same way as you might ask, did you like the graphical user interface? You can ask the user, did you like the chatbot? They know that the system was a chatbot. We're not trying to say, did it fool you into thinking it was a human, but simply, how good was it to use? How easy was it to use? Do you think you'd use this system in future? And this is much more common for practical chatbots for specific purposes, like the IT help desk chatbot or some of the example chatbots you'll see on the videos. OCOM5204M – Data mining and text analytics © University of Leeds 23 of 27 Other things which aren't really measuring the humanness of a chatbot at all, but things like efficiency cost. How long did it take? Or quality cost, how many times the system had to say I don't understand you, number of times the user had to barge in. So, these are more or less systems engineering evaluation or IT systems evaluation metrics applied to chatbots. These aren't really chatbot specific evaluation measures. So, that's how to evaluate. The answer is we're not very sure, but we can evaluate them in the same sorts of ways as you evaluate other IT systems and IT interfaces. Finally, I want to look at some ethical issues if you're using an IT system, then you have to...as an interface, then a general way of building interfaces is to study the users and the task and build some sort of simulations. A Wizard of Oz study is based on a famous story of Oz where the wizard of Oz appeared to be an allpowerful wizard who knew many things, but actually turned out to be just an American salesman. So, the salesman behind it was just an ordinary person. So, in the same way, a Wizard of Oz study is having a real person pretending to be the chatbot and entering into conversations. And then you have conversation transcripts between human and pseudo chatbot to give you an idea of what the chatbot should be talking like, so you set iterative tests on users. And one issue is, particularly if you're having a Wizard of Oz study, then the users get used to the idea that it is actually very human-like because there really is a human there. So, distinguishing between humans and chatbots becomes important. In artificial agents you have to make clear that you're not simply building it to see if it's possible to build. An old example before there were computers, Mary Shelley wrote this book called Frankenstein where she created a human out of bits of other humans, sewing them together without consideration of any ethical issues. So, just the idea was the doctor wanted to find out if it was possible to take bits of both human bodies and sew them together to make a human again. And of course, when they did it, when they succeeded, the human was very upset because they didn't feel normal, didn't feel right, and lots of things happened in the story which weren't very good. OCOM5204M – Data mining and text analytics © University of Leeds 24 of 27 So, there is a problem with a chatbot. There are various ethical issues. There are certainly safety issues, for example. If a system starts distracting the driver while they're driving or if it's a medical chatbot and it starts giving bad medical advice or if a system is being used by psychology as psychotherapy for mental health patients and the system starts being nasty or abusing the users, then they could be put in danger. There's also if a system starts being biased or nasty, not just about the user, but about particular social groups, then that's representational harm. And of course, there's also this problem of information leakage, that the chatbot, if it's trained on a corpus, it knows what's in the corpus. It may know things about the people, what they said in the corpus, which may leak out. If somebody in the training corpus read out their Social Security number, then the system may say their Social Security number back to the user. OK. So, as I said before, it's very important if you're using a chatbot for mental health but you don't say anything, it's going to be bad for the mental health, or for in-vehicle conversational agents. If you come to Leeds University, we actually have a driving simulator, which is a real car where they've taken out the actual mechanics and put in computer graphics and computer-generated movement, so the car moves around and stuff...so it feels like it's moving around anyway. And this is used for things like testing out interfaces to different computer control systems. And when I've tried it, I have been aware that it is a virtual system and I've also been very aware that this chat thing talking to me is distracting me from driving so that I crash into things. Luckily, they're only virtual lorries that I crash into. So, you have to be aware of the environment and the driver’s level of attention in developing the chatbot. Here's a real example. Microsoft developed a Twitter chatbot, and they gave it the personality of a young 18 to 24-year-old American woman. They trained it on lots and lots of Twitter data from such people and it was allowed to tell jokes and asked people to send selfies. OCOM5204M – Data mining and text analytics © University of Leeds 25 of 27 It used informal language and slang and emojis from the training corpus. They found a training corpus. And it was designed to learn from the users so people who used it, their interactions were added to the corpus. However very soon, Tay became offensive and abusive, and it started giving out conspiracy theories and Nazi propaganda and harassing women and reflecting racism and misogyny, and that's because it was reflecting what was already in Twitter. The problem is that there's no filter on Twitter. Anybody can post anything on Twitter and people were posting misogynistic, racist, and inflammatory tweets, and therefore it was learning them and spouting them back. So, Microsoft had to take Tay down after only 16 hours. So, you have to think about...in the design phase you can't simply assume that whatever the users say should be included in your system. You have to have more filters about this. And this pointed out the issue that actually all trained data sets, if you take real data, is likely to have biases in them. So, they try and take applying hate speech and bias detectors on training data sets for dialogue systems. Twitter, Reddit, and other dialogue data sets have already said how useful for training data, but they found lots of bias and hate speech in the training data. And therefore, because it's in the training data, the dialogue models trained on this also include hate speech and bias. A quite standard example, the British National corpus, 100 million words, was developed by a number of researchers. One of the researchers in this made quite a name for himself by writing a series of research papers pointing out or built on the essential thing that, OK, the most frequent words in the British National corpus are function words like the, often, and. The most frequent content word which isn't a function word is actually a F-U-C-K or fuck used as a noun or verb or an adjective. And that's because in real British English that happens to be a very frequently used word. However, if you have a conversational agent or any other chatbot, if it keeps on using that word that may be unacceptable to certain applications. OCOM5204M – Data mining and text analytics © University of Leeds 26 of 27 OK. Another issue is privacy in the corpus. There may be things like computer, turn on the lights. And then if you answer the phone, you may say, hello, yes, my password is...or yeah, the user may be out buying something and therefore gives out his credit card numbers. That happens a lot in real corpus. So, I've listened to transcripts of telephone conversations which include these sorts of things. Oh, sorry. I've not listened to; I've read transcripts of conversations that actually include these sorts of things. There may also be intentional information leakage. Dialogue systems designed to send user data or dialogue systems are designed to ask the user to give them their credit card number. If you want to buy something you have to give your credit card number. So, it's important that if you're going to use this training data then you have to preserve the privacy in the training by removing it or blanking it out in some way. OK, so that's some of the ethical issues in chatbots. So, to summarise then, we've had an introduction chatbots and dialogue systems. We've seen that chatbots are...you might think of them as just basically the user says something, a chatbot replies, or a user ask question, the chatbot gives the answer. Human conversations are much more complicated. There are turn taking like question and answer, but there are other sorts of turns. There's dialogue acts. There's also things like interruptions. You have to decide when to stop, when to give way and let the other person have a turn. There are essentially two general sorts of architectures, rule-based systems or machine learning systems. The rule-based systems humans have devised long sets of rules, and we've seen some examples of this. Or the corpus-based chatbot, you get a large dialogue corpus and you learn from that in one way, either by instance based learning...basically having all the dialogue act as being examples...or by a neural network, building a model off the corpus and then generating the response from that model. Nowadays current commercial systems like Alexa have some sort of combination of corpus-based modelling and also state based modelling, so you have a dialogue state architecture. For chatbots, there's a real issue in evaluating them. Its usually humans are involved in some way. And as with OCOM5204M – Data mining and text analytics © University of Leeds 27 of 27 other text analytics systems, we always have to be aware of ethical issues. Chatbots seem to be human-like and interacting with chatbots we take it for granted that they're going to be human-like. But really, we also have to be careful that they don't start spouting Nazi propaganda or giving out personal information that they shouldn't do. OK. For more information, please read the textbook chapters on question answering systems and on chatbot and dialogue systems in the Jurafsky and Martin textbook. Also read some of the papers that are recommended because there is some useful background on ALICE and Hubert and AIML and so on. OK. Thank you very much for listening and I hope you enjoyed the lecture. Bye for now. 
--------------------------------------------------------------------------------------------------------------------------
OCOM5204M – Data mining and text analytics © University of Leeds 1 of 5 University student surveys using chatbots: Artificial Intelligence conversational agents Professor Eric Atwell: What I want to do today is to show you something about our research on chatbots. I wanted to ask you to have a look at a research paper. We have a European funded research project on looking into the use of chatbots in higher education. And I'm going to quickly go through some of the highlights so you can see what you want to read. So, you might think a chatbot is simply something where I say something, and the computer says something, and I say something, and the computers says something to chat. But actually, the dialogue is much more complicated. You have to keep track of other things that are going on and ideally, you want to have interfaces to various other IT systems that are going on. So, here's the paper. University student surveys using chatbots: Artificial Intelligence conversational agents. I'm just going to go through some of the highlights in the diagrams and then you can read it yourself later on. So, Noorhan Abbas and Tom Pickard, they're research fellows on the project. And Aisha Walker is my colleague in the School of Education who's involved in using chatbots for education. And I am a computer scientist. The abstract is essentially that chatbots are useful in many applications. And ideally, we want to be able to use chatbots for one exact particular example. And it's the course evaluations. So, at the end of the course, for every module at Leeds University, you get sent a questionnaire and you have to fill in or for each of the questions answer on a scale of I think it's 1 to 5 whether you agree or disagree. The trouble with that is that you don't get much scope for really useful feedback. If you just know a scale of 1 to 5, OK you know overall people like each of these things or don't like these things. Whereas what we'd like is to have some more narrative texts and real details of what you like and what you don't like. So, that's what the chatbot tries to do. And this has been used in four different universities and research centres in the UK, Leeds obviously, in Spain, in Croatia, and in Cyprus. The chatbots were developed by commercial companies in Norway and Sweden and then we use them in our universities. And we found that collecting coarse feedback this way gets better quality responses and potentially the students can think it's fun as well. OK. Let's go down and have a look at some of the highlights of this. So, first of all, there's some research on getting feedback from students. If you just have a question and you have to respond it by a tick box, then there's two problems. One is that you don't get that much useful information out of it. Another problem is that some students will just get fed up with having to answer these things. If you get loads of questionnaires and each questionnaire's got 10 questions on it, then you may not respond at all. So, we end up getting very poor-quality responses compared to a face-to-face conversation or interview or even a telephone interview. OCOM5204M – Data mining and text analytics © University of Leeds 2 of 5 So, maybe what you could have...the trouble with telephone interviews is they're very time consuming and expensive to organise so why not have a chatbot do it instead? That was the overall idea. And that's the system called Hubert or Hubert.ai And ai is the top-level domain for AI research, apparently. And there are a number of other chatbots around. So, the first thing we do is have a look to see what other sorts of chatbots are there available for this sort of research. So, very prominently, Amazon and Google and Microsoft have voice-driven chatbots. So, Amazon have Alexa, where you can speak rather than having to type things in. And are going to look at some of these examples in a minute. Amazon Web Services have a demonstrator called Ask LU. And LU stands for Lancaster University, not Leeds University, unfortunately. And it was developed where you can ask questions about your timetable or grades or when is my next lecture or things like that. It's not actually teaching but it's providing services. And there are other chatbots which provide services. Many of these chatbots, or chatbot architectures, were originally developed for commercial services. So, for example, I have a NatWest bank account. And when I opened my online banking, there's the little Cora chatbot and I can ask questions like, I don't know, how do I close down my account? Or how do I make a complaint about NatWest? And sometimes it will give me a right answer. And within universities, they realised that these sorts of architectures could be adapted to many student services. So, for example, Lola or Dena, there are systems for enrolment and admissions when you first join the University. Differ is a system that we're using on our research project, which helps students to, even before they come, to get to know each other. So, it helps with student engagement, getting students to feel that they're part of the University community. But that's not really teaching. So, there are also some researchers developing systems for helping to teach and learn. For example, Coding Tutor. This helps students in introductory programming courses. It sort of analyses the software and has an English language feedback about what you did wrong. You're probably aware that here at least, we do have automated grading systems for Python programming so that you can do the Python course then some of the grading is semi-automated. But the grading, the results are then said to the tutor, who then gives you a mark, whereas the Coding Tutor chatbot in addition gives feedback to the students in English that you could have redone it this way or something like that. And there are other teaching chatbots, like Clive chatbot and Book Buddy chatbot for helping certain teaching tasks. The problem with teaching is that this is something we discovered that real teachers like me aren't just answering your questions but are doing a bit more than that. So, building in the background knowledge into that sort of chatbot can be harder than building the background knowledge for a chatbot which tells you when your next lecture is. Many of these things, sort of services, are typically done by access to databases of some sort, which you can imagine doing, having a chatbot interface to a database, whereas teaching and learning requires more knowledge somehow. OK. So, what we did for our project was we tried to find one use case, which was fairly straightforward, which would be useful in teaching and that is getting feedback from students about the course. What do you think of it so far? How could we improve it? What should we be dropping from it? So, we had partners in the project University of Granada in Spain, University of Zagreb in Croatia, and the Centre of Excellence Centre in Cyprus, Science Centre of Excellence, and Leeds OCOM5204M – Data mining and text analytics © University of Leeds 3 of 5 University in the UK. And we all taught courses and at the end of the courses asked the students to give us some feedback on the courses. A lot of times, the students didn't reply at all, and we got...most of the courses, because the project is in chatbot, most of the courses are computing-oriented or business-oriented because that's the sort of area where chatbots are taught so it's not all University students but a selection of classes in computing and business sort of areas. And there's a sample of how the system went. Hang on. Let me just try to make this a bit bigger so you can see a bit more clearly. "Beep boop. Here we go. By the way, just say 'go back' and correct me if I misunderstand you. Hi how old are you? Please type your age as a number." And then the student has to reply. And "What is your gender? Please choose male or female." This time, those are the only options. So, you can see what is working well with course X and should continue in the same way, where course X is replaced with what the actual name of the course is. And the student is supposed to type something, not just yes or no or a score but more about what is actually good or bad. And then what could the teachers start doing? What could improve it? And what could the teachers stop doing that could improve course X? So, the notice that it's asking the same questions to all the students but there will be some variety. And also, this is just the overall framework. So, if a student doesn't give you an appropriate response, then the chatbot can go into a dialogue to tease out more responses. So, that's the big difference. It's not just presenting the questions in a box for you to type into, but it is actually going into a conversation. And we'll see some examples of it in a minute. So, we managed to get quite a few examples from a number of different courses. You can see here, mainly in computing type areas or in business type areas, like taxation and business informatics. So, these conversations were of varied length. So, here's the average depth of conversation. How many interactions were there? So, we saw from the template, you're basically asking a small number of questions but sometimes the student didn't give an appropriate answer or just said, if I asked you what was wrong with the course, if a student just said, nothing, well, the chatbot would try to say something like, are you sure? Can you tell it think about it a bit more? So, in drawing out the conversations, we saw it went up to about 30 turns, which is quite a long conversation. Here's the ratings that the students gave back at the end of the chatbot asked, what do you think of course overall on a score of 1 to 10? And the students tended to like the course so most of the scores are around 9 or 10. The chatbot did try to get out more detailed, for example, are you absolutely, definitive 100% sure about that? And a smiley face to make sure that everything was OK. Here's an example of a somewhat longer conversation just to illustrate that. Hubert says, "What could the teachers start doing that would improve it?" User replies, "I think everything is all right. Nothing to improve." Hubert says "Surely there must be something that could be improved." User says, "I can't remember anything bad." "Not yet? Let me repeat the question. What could the teacher start doing that would improve it?" "I can't remember anything bad." "Not yet. What could the teachers start doing that would improve it?" In the end, the user is sort of cajoled into saying something. Maybe weekly tasks could be equal because they heard that some students got harder tasks than others. So, there, Hubert's actually trying to not just simply accept that the student is very happy but try to find something. So, it goes on OCOM5204M – Data mining and text analytics © University of Leeds 4 of 5 a bit more. And sometimes by the end of it, the user is drawn out into a conversation and says quite a lot. So, you can see that this is quite detailed feedback. And this is useful for the lecturer because they know what sorts of things they should at least improve for next year. Potentially, you could ask this feedback question at partway through the course. But typically, what happens is you get some feedback, and this goes into teaching next year. And of course, it goes back into, I have an annual academic review with my head of school and head of teaching and learning. And they get to find out something about what really works and what doesn't work for them in terms of not just this one module but all the modules I'm teaching. And as well as one other thing we tried to do is well, is saying, what did you think of course and what did you not think of course? At the end, we asked, OK, how did you like this type of evaluation compared to a regular survey? That was, what did you think of Hubert? And the user may well say, very interesting and unique or things like that. And we also try to get a score for that. On a scale of 1 to 10, would you give it a 10? And then the user generally thinks it's fun or it's nice. So, you can see some of these, as well as asking about the course, we asked about Hubert. Because we want for the European Union, they want to know...they're funding the project...they want to know if it's very good. Yeah, the depth of the conversation is the count of both the questions and responses. How many terms were taken? So, it does include Hubert's turns as well. So, let's say...I suppose you want to say how many things did the student say? More or less half that. But the conversation is both turns. This user said "This way is totally different with regular one. However, it is interesting." So, they liked...it's really interesting. Better than filling a form. So, the students like this and furthermore, they're more likely to interact with this. If they're given yet another survey questionnaire form to fill in, they might just ignore it or not answer it. One unexpected side effect is that because we were asking computing students, some of them started to say actually, I just want to see if I can break the chatbot. They answered the questions, yeah, but they were also interested in trying out the chatbot and trying to see if it was really intelligent by asking difficult questions. So, we have some examples of having answered the questions, Hubert says, “Thank you. Those are all the questions I had. You may close this window now. Have a great day”. The user says "bitch" to rile the chatbot, see what happens. Hubert comes back and says, "A poor choice of words never improved anything." Hubert the bot, he knows that bitch is not a nice thing to say. So, the user says, "Tell me a joke." Hubert does tell them a joke. "Knock." "Who's there?" "Hey there. I don't have any questions for you right now. Please come back later." The user doesn't think that's a very good joke. "Sick joke, man." But at least he appreciates it is a joke. Sometimes the user tries to ask Hubert question just to see what will happen. So, here we have Hubert says, "I don't have any questions right now. Please come back later." The user says, "coronavirus. What does coronavirus cause?" The user does know what coronavirus causes. They just want to see if Hubert knows. Hubert says, "I'll try to answer that. May be totally wrong. Give me a sec. SARS-CoV-2 virus." So, Hubert has got some sort of answer, even though it's not a very sensible answer. We get longer examples of conversations. Here's a clever one. User says, "What is it?" Silly question. Hubert says, "I'll try to answer that, but I may be totally wrong. Give me a sec. What Is It is a 2005 OCOM5204M – Data mining and text analytics © University of Leeds 5 of 5 American surrealist film written, edited, co-produced, and directed, blah, blah, blah.”? So, even a silly question like what is it? Hubert can come back with, what it's basically done is done a Google search of "what is it" to try to find some answer to that question. So, there's examples of student responses that were challenging for Hubert. So, in general, it worked out nicely. Another nice feature is that students could change their minds and modify their answers. In conclusion, the chatbot was able to guide the students to much more constructive answers and many students volunteered positive perceptions of the use of Hubert. They really thought it was good using Hubert rather than just having a questionnaire. And students would engage with the chatbot and even give friendly responses to Hubert and Hubert was friendly in return. Sometimes students try to break the system. They would try to give challenging responses and then Hubert tended to either just give a silly reply or treat it as a joke. Or Hubert could ask them to modify their questions to see if it worked. So, at least one of these responses, one of the interactions was very long and it was just the student keeping over and over again, trying to break the chatbot. OK. I think that's it. Thank you very much for listening. Goodbye. [END]
-----------------------------------------------------------------------------------------------------------------------------
OCOM5204M – Data mining and text analytics © University of Leeds 1 of 26 Information retrieval and web search Professor Eric Atwell: Hello, this is Eric Atwell, and in this lecture, I'm going to give you an introduction to information retrieval. Nowadays, it's commonly thought of as being Google search or web search. You type in some keywords, and there comes the pages you want. But there's a longer history of information retrieval where you type in some keywords, and you get the documents that you want. So, in this lecture I'm first of all going to compare information retrieval with database querying using something like SQL, or structured query language. Then we're going to look at a particular format, the inverted file format, which makes the matching much more efficient between keywords and documents. I'm looking at two different models for doing this matching. One is a Boolean set theoretic model and then secondly, more realistically, a weighted vector model. That's more like the model that Google and other search engines use. We're going to look at a couple of very simple worked examples to illustrate how this works and look at, how do you evaluate a search tool or an information retrieval tool? And then finally if you're not getting back sufficient, good answers, how do you do query broadening to improve the matching and get more of what you want and less of what you don't want? So, let's first of all compare database querying with document search or web search. Well, in a database, the records are made up into fields. And a simple approach to searching for a particular keyword or string of characters would be to look for in any record, in any field, the string with wild cards before and after. So, if I want to find the word "graphics”, then I can search for percent graphics percent, where percent mean match anything. OCOM5204M – Data mining and text analytics © University of Leeds 2 of 26 But this is rather inefficient because it basically means we brute force scan to go through all the text data records until you find one or more of these. And this is because the information is stored in a structured manner. That's the whole point of a database. So, you might say, well, if you want to find information, why not do information extraction? That is, rather than hold the full text, do the information extraction or content analysis, extract the named entities and keywords effectively, and hold these in a relational database. So, for example, the University of Leeds module catalogue, which has information about what modules are taught, what semester they're in, and other stuff like that, you can imagine having some sort of structured database which has modules indexed by index terms. And the modules would have information like the module code, the title, semester. The terms would have an ID and a value, and you'd have an index which links the module code to the term ID and so on. And now imagine that you want to find all the modules which are related to a database or AI or knowledge base. So, to find all the modules, here's a bit of SQL to do it. And it's quite complicated already. You've a select distinct m from module...well, I'm not going to read it all out. But the point is you've got to find a term value is database or a term value is AI, or a term value is knowledge base. And what about if you want to have a course which looks at...which includes database and AI and knowledge base? We might think just change the OR’s to AND’s to get that. That doesn't actually work because it's saying the T-value has to be database and at the same time a T-value has to be AI. That's not possible. T-value can only have one value. It can't be two. It can be database or AI, but it can't be database and AI. So, this SQL doesn't work. If you just want to say two terms, database and AI, then even that is quite complicated. What you actually have to do is to have two T's, T1 and T2 and specify when T1 value is database and T2 value is AI and find all the records or fields containing T1 and T2. So, it gets quite complicated if you want to search a database for several keywords...in other words, you want to find records which have got several keywords in them. OCOM5204M – Data mining and text analytics © University of Leeds 3 of 26 And that's why database querying using SQL isn't a sensible way. It gets much more complicated for "and" than it is for "or”. And the trouble is in information retrieval, you typically do have several keywords, and you want to find all the documents which have all of them, not just a document which is any one of them. So, instead of having a database stored in records and fields, you have a non-database structure. And therefore, it's not suitable for standard SQL. You have to think of every word, every index term, as a possible entry point to a document. There are document record identifiers, and each document has a number of terms in it, a number of words in it. And what we actually want is for each word, a number of documents it contains. So, this is the standard indexing method for information retrieval systems. And it's more or less something that...something like this is used in most search engines. And that's called an inverted file. So, the basic idea of an inverted file is, well, you start off with a number of documents. And document 1 will have term T1.1, T1.2, and so on. Document 2 would have term T2.1, T2.2, and so on. And what you actually want is for each term, a list of documents it contains...so for term 1, which documents that's in, for term 2, which documents that's in. So, that's the notion of inverting. So, having got the set of documents, you then have to do some preprocessing to extract from all of a set of documents a dictionary of all the words that are in all the documents and for each word in the dictionary which documents it came from. And once you've got that dictionary, then you can much more efficiently find things. Here's a dictionary, and the dictionary will have term 1, term 2, and so on. And it may have for each term how many times occurs. And then the next item 1 is a link into where in the postings file, so index where it is. OCOM5204M – Data mining and text analytics © University of Leeds 4 of 26 So, let's say for example we want to find examples of term 1, which might be the word "a”. And there's two examples of the word "a”. We start it post position number one. And since there's two examples, it must go from one and two and then stop at three. And then we could just follow, where does 1.2...well, data file number 1 and 2 is data file number 2, so document 1 and document 2 have got the word "a" in them. Next of all, let's find term 2, which might be "and”, let's say. And the word "and" appears three times starting at document position number 3. And there's three of them, so it's three, four, and five. And it ends at number six. So, three, four, and five values here, three is document 1, four is document 2, and a five is document 3. So, it appears in documents 1, 2, and 3. So, we know that "a" appears in document 1 and 2, and "and" appears in documents 1, 2, and 3 and so on. I can run on term three, which only occurs once, let's say aardvark. And this is position number 6. And its document number 2. So, "aardvark" appears in document number 2 and so on. So, a dictionary is a list of terms including normalised keywords or stems. So, you might have "aardvark" but not "aardvarks" because "aardvarks" is included in "aardvark" and the frequency with which the term occurs. So, "aardvark" only occurred once, whereas "and" occurred three times and a pointer into this inverted file and then the access to the dictionary. If you want to look up a word like "aardvark" in the dictionary, you just use some standard file access method like a hashing algorithm. So, the inverted files got for each entrant a dictionary a list of pointers into a data file identifying those objects. And the inverted fire may also contain other information like positional information, where in the document it is. In this example so far it hasn't, but we've got it. And the term frequency, how many times it occurs in a document, that is potentially useful because if there's more frequent occurrence, it's perhaps more important or perhaps less important. OCOM5204M – Data mining and text analytics © University of Leeds 5 of 26 So, let's try a query like A or B and C. Let's say it's a cold day out and I want to make some pudding. And I want jam pudding or treacle pudding. So, I want jam or treacle and pudding. So, that's my query. I want to find all the documents which have got jam or treacle and pudding in them because I want to make a jam pudding or a treacle pudding. And what you have to do is...well, the algorithm first of all is this rather complicated thing with open brackets, A or B, closed brackets, and C. It's too complicated. So, you have to convert it into what's called disjunctive normal form. That is x or y or z or x or y or z or other things. And then you can basically try matching each of these one by one until you found what you're looking for. An A or B and C amounts to A and C or B and C or possibly A and B and C. Those are the three options that are matching. That was 101, or 011, or 111 in binary terms. And then, the T... then we have a list of documents. And the first list is the ones containing "a”, and the second list is lines containing B. And the third list is the one containing C. So, in order to find a match, you basically have to try this disjunctive normal form against this list of three lists. So, the very first one, 101, is matches...it's in document 1. And it's not in document 2. Well, 101 matches it. It is found. So, the A is in document 1, and C is in document 1. That's good. So, then we throw away document 1 or pop it off a list. And the next thing is document 2 has a profile 011. And then we pop off document 2. And document 3 has a profile 110. And we pop off document 3, and document four has a profile 101. And we pop that off, document four. And we see document 5 has a profile of 010 and so on. Document 6 isn't...has also got profile 010. Document 7 is 101. 8 is 110. Document 9 is 001. Document 10 is 100. Document 11 is 001. And document 12 is 011. So, OK, for each document, we have a straightforward profile which corresponds OCOM5204M – Data mining and text analytics © University of Leeds 6 of 26 to whether or not A, B, and C occur there. And then we basically see, does it match 101 or 011 or 111? Having done all that, we see those are the only actual documents. Document 1 matches 101. Document 2 matches 011. Document four matches 101. And document 12 matches 011. So, we've found four documents which have A or B and C. And it was documents 1 and 4 have got jam pudding. And document 2 and 12 have got treacle pudding. So, there's our answer. And we can report the number of hits to the user. We found four documents, and we don't even have to actually retrieve the documents. This is only in terms of matching index terms. Then the user can say, yes, I like these and then go and get the document, so document 1 document 2, and so on, effectively URLs. And then if you want to click on them, then you can go and get the documents. Of course, that's simply the word is in there or not in there. And that's not realistic. So, what's actually important is weighted weights. All the words have weights, and the words in the query have weights. So, instead of saying A or B and C when you issue a query to Google, what we actually want is, let's say, I want jam. I'm not 0.5 as my weight for jam. I'm very even keen on treacle. And C, I must have pudding. So, this is a vector for my query now. I'm saying, I probably want jam. I even more surely want treacle. And I must have pudding. So, all the weights are in the range of 0 to 1. So, any other word has got a weight of 0 by default. We still have a list of each block for each of these key words, which documents they're in. Remember, this is the inverted file for each word. OCOM5204M – Data mining and text analytics © University of Leeds 7 of 26 For A, we have a list of documents it appears in. And we also have a weight of that word within the document. So, for example, if the word A jam, in document 1 it has a weight of 0.2, whereas in document 3 it has a weight of 0.6. Basically, it means jam is much more important in document 3 than it is in document 1. Where do we get these things, these weights within the documents from? Well, it's something to do with a frequency of a document or the frequency of the word in the document compared to the frequency of your other words, something like TFIDF within a document. And I haven't got time now to go into more details. Have a look at the textbook, this Jurafsky and Martin textbook chapter on question answering does have a section on information retrieval and which will explain this a bit more. So, we have to assume we have some sort of similarity function, which for any query and document comes up with a score by comparing these vectors. So, again, we can look. We can go down, do the same thing we did before, get a profile for each of the documents. So, profile for document one is 0.20.4. And the similarity between the query, 0.5.71, and document 1, 0.204, using this function comes up with 0.85. So, what is this function? Well, it's cosine function that we saw earlier. And we get the same...do the same calculation using cosine function for document 2. Again, compare the similarity of the weighted query vector 0.7 with the document vector, which is 0.6.4. And this time, we get an answer of 0.6. That means that document 2 is slightly better than document 1 for our query and so on. The last thing about this, once we got these weights attached you can sort or rank the list according to these weights. So, we just said that the document 2 is better than document 1. So, it's presented to a user first. OCOM5204M – Data mining and text analytics © University of Leeds 8 of 26 We present the ranked list to the user in rank order. And you can say, I only want to have, let's say, the best two, in which case all the others that aren't in the best two are not offered. So, you just offer the URLs for the best two. And then you can say, if you want, you can get the next. So, far, we haven't actually retrieved the documents. All that's stored in the inverted file is for each word a list of document URLs and the weights of the words within each of those documents. So, we don't actually have to start downloading these things. Furthermore, you can do something a bit more complicated now. If you want to say not just, I want A and B, but I want A within three words of B or I want A before B, if you carry information in the listing in the file...in the index, sorry, about where the positions of the words are, then you may be able to do things, for example, to search for "Venetian blind" rather than "blind Venetian”. And in principle, this is possible. I won't go into details of how it's done. But this does become important. You want a query for "Venetian blind" to favour documents which actually have the phrase "Venetian blind" in it but still allow it to find "blind Venetian" as a sort of second best if you found enough "Venetian blind" documents. So, we can see that the inverted file can be used for Booleans, but also weighted and also even these positional queries, and query processing can be completed without having to upload web pages. Before the query is launched, you have to do a lot of pre-processing to create this index, so Google has to, before you ever take a query from a user, set up this huge index of all of the documents in the world for all the vocabulary that appears in all of those documents and then for each vocabulary item which document appears in and where the words are within that. So, it's got a huge amount of pre-processing to do. But once that's done, the query and query processing and matching is very fast. The number of hits is available straightaway. It can be expensive to update if the information objects change. OCOM5204M – Data mining and text analytics © University of Leeds 9 of 26 In other words, if web pages change then you have to essentially recompute the entire index all over again. And unfortunately, web pages do change. So, Google has to keep maintaining its index. Google is very demanding in storage requirements. The dictionary plus the inverted file can take up about the same size as the original data. So, you have to store the contents of the web pages and you also have to, because you need the [INAUDIBLE] of the web pages, to extract this dictionary in inverted file. But you have to store that too. So, an inverted file index is a standard system for information retrieval. Relational databases aren't very suitable for doing this sort of thing. Standard SQL is not very good at expressing queries, find all the records which have got several keywords in them. That doesn't work very well in SQL. The inverted file structure is much more efficient, and it can also store frequencies or weights in the dictionary to allow you to order the results rather than just saying, I've got four matches. You can get a score for each of those matches, and then you can put them into some sort of ranked order. And this allows you to find quite subtle things like the difference between "knowledge management" and "management knowledge”. If you query both of those things to Google, you may find similar documents but in a different order. And remember, Google has to find documents very quickly. I say Google. So, do the other search engines too. It just happens that Google is probably the most famous at the moment. This is done very efficiently. And so, the cache, basically the Google robot goes out on the web all the time finding new web pages, storing a copy of a document and also a cut-down version of a document. When I say document, I mean any web page. That counts as a document. It has to keep the words in the document and a stored, sorted list of the words appearing in a document with links back to the full document. So, that's done via this dictionary and inverted file. So, there it is again. OCOM5204M – Data mining and text analytics © University of Leeds 10 of 26 So, we've said that information retrieval is quite different from database management systems. It's different in a number of ways. It's different in the underlying algorithms that are used, but it's also different in what is expected of it. So, for a database management system, you expect to issue an SQL query and get exactly what you asked for. With information retrieval, you type some query into Google, and you get a partial or best match, and you get a rank ordering of matches. It may be some of the lower-down matches are very partial and not matching completed at all. The inference mechanism in database management systems is deduction. Whereas information retrieval, it is induction. If you're a philosopher, this can be important. Probably, it's not that important for computer scientists. But what is important more is that the underlying mathematical model is deterministic for database management systems. That is, if you issue a query, you should be able to predict exactly what you get back. And the same query you should give you back exactly the same information every time. Whereas information retrieval is more probabilistic or that the weights attached involve random probabilities to some extent, which means you may not get back the same query, the same responses, or the same ordered responses the next time you issue the query. The data in database management systems is structured into records, and the records are structured into fields and subfields. So, it's very structured. Whereas information retrieval, you basically have a set of documents, and we don't know anything about the internal structure of a document except that they contain text. And the query language of SQL is pretty artificial. I find it quite hard to think about. Maybe after you've learned it, maybe you can get used to it. OCOM5204M – Data mining and text analytics © University of Leeds 11 of 26 For information retrieval systems, Google lets you say in sort of natural English. I say question mark natural because you have to think of some key words and then type them in. You can issue a question to Google like, who is the queen of England? And then Google will try to find an answer to it, will treat that as a question rather than a query. So, it's sort of quasi-natural. The query specification, as I said, has to be exact and complete for SQL. Whereas for information retrieval, you can have a guess as to some keywords. And it may not be the complete, best answer. But you may find what you want even with just an incomplete query. And that's because you don't necessarily want exact matches. The items that you want from an SQL query are exactly matching the query. Whereas for information retrieval, what you want isn't actually a match to the keywords. It's got to be something relevant to what's in your mind. So, if I'm looking for...let's say I want to find out the Russian word for "table”, I might put into Google "Russian table”. And it comes up with answers. And the answer is correct if it gives me the Russian word for the word "table”. Whereas my friend might be looking for a table of Russian-to-English translations. And then he puts in "Russian table" into his query engine, and he gets back a translation table of Russian words and of English equivalents and then he's happy. So, relevance is dependent on what you actually want rather than what the exact match is. The error response is also different. If you put in an SQL query and it's not quite right, then you'll get something completely wrong. So, it's sensitive to errors. Whereas information retrieval, if you type in a query and it's not quite right, you'll still get more or less a match. So, it's relatively insensitive to errors. So, Google is more error resistant or error insensitive. Information retrieval was first of all for libraries, for bibliographic systems. So, they still refer to documents as what you trying to look for. It's not just web pages. OCOM5204M – Data mining and text analytics © University of Leeds 12 of 26 And the idea is you'll have your representation of a document as a set of descriptors or index terms or words in the document. So, you are searching for a document in the space of index terms or words. So, we have to have a formal language for formulating queries and a method for matching queries with document descriptors or keywords. This is the general architecture. The user types in a query of keywords. The query matching looks at the object base. That is the database of the base, the record of all the web pages, all the documents. And it finds ones that match and returns hits. Now, sometimes the user may say, I didn't like these. But I would more like this one. That's some feedback. And there may be a learning component which lets you give better hits and more of what you want and less of what you don't want. We look at query broadening a bit later on. So, let's look at some worked examples of Google in a very simple world. Let's say we have a very simple set of documents D and a very list...small vocab T of terms that you're allowed to. So, for any particular document, the document is represented as a list of the words that are in that document. And so, let's just say, for example, there in our very simple world wide web we've only got three documents, D1, D2, and D3. And the only words of interest in any of these documents...let's ignore all the function words. The only interesting words are "pudding”, "jam”, "traffic”, "lane”, and "treacle”. So, pudding, jam, and treacle, you may remember from my earlier example. It's cold outside, and I want a jam pudding or a treacle pudding. But the documents can also contain "traffic" and "lane”. So, here's these three documents. The first document 11000 represents the document. That means it contains pudding and jam and does not contain traffic, lane, and treacle. The second document contains traffic and lane, but not the other words. And the third document has got pudding, jam, traffic, and lane, but not treacle. So, what are these documents? OCOM5204M – Data mining and text analytics © University of Leeds 13 of 26 Well, let's just imagine the first document is a recipe for jam pudding. That's why it's got jam pudding in it. The second document is a Department of Transport report on traffic lanes. That's why it got traffic lanes in it but not the other words. What about a document which has got pudding and jam and traffic and lane? Well, this is a radio item on a traffic jam in Pudding Lane. Pudding Lane is as a road in London where the Great Fire of London started. So, I've actually been to Pudding Lane to have a look, and there's lots of traffic there all the time because it's in London. And there could well be a traffic jam. Notice that this simple representation does not capture any bigrams. It only captures unigrams. So, it doesn't capture the idea that jam should go with pudding and not with traffic. So, those are the three documents. Now we want to find our recipe for something to eat tonight. Let's say in the Boolean model the query is something like I want jam or treacle and pudding and not lane and not traffic. And I've got to convert this into this disjunctive normal form. Our vocabulary is pudding, jam, traffic, lane, and treacle. That means I want jam. I want pudding and jam, but not the other ones. Or I want pudding and treacle, but not the other ones. Or possibly, I want pudding and jam but not the other ones, so 11001 represents pudding, jam, no traffic, no lane, and yes treacle. My original query was I want jam or treacle and pudding and not lane and not traffic. But we can't have these AND’s. We've got to really express it in terms of OR’s something or something or something. So, then I can quickly match against the internal structure. So, to match a document with a query, you have a similarity function. And since this is the Boolean world, the similarity function is yes, 1. The score is 1 if a document equals 1 of these query vectors, if it's either 11000, or 10001, or 11001. Otherwise, it's 0...a very simple similarity function which is either one or 0, depending on whether or not we found an exact match or not. OCOM5204M – Data mining and text analytics © University of Leeds 14 of 26 So, let's take this in a Venn diagram to show the possible combinations. The first document is 11000. Does it match this or this or this? Well, it matches the first thing, 110...it matches the first component of the query. Therefore, document 1 is a match. The second one is 00110. It doesn't match that. It doesn't match that. It doesn't match that. Therefore, that's not a match. The third document 1111...sorry, 11110, it doesn't match that. It doesn't match that. And it doesn't match that. Therefore, that's not a match. Therefore, we found the first document is the only thing that...it's a jam pudding recipe. And that will do for us. So, we found by collecting the results the answer is that document 1, 11000, which is the jam pudding recipe. So, I'm happy. I have a jam pudding recipe to cook myself a nice pudding tonight. And if you like Venn diagrams, there's the intersection. That's OK if you only have 1's and 0's. But in real life or even...this is a very simple world. But we're still going to insist on having weights. We want vectors with numbers and weights in the range of 1 to 0. It's not by binary value. So, the query also has to be as a vector. Remember, there are five terms. And the first term, remember, was "pudding”. I must have pudding, so pudding gets a weight of 1. I'm quite keen on jams. So, that has a weight of 0.6. I don't want traffic or lanes, so they get 0's. I like treacle pudding even more than jam pudding, so that gets a weight of 0.8. So, that's my query. 1, 0.6, 0, 0, 0.8 is what I want. And the documents also have for each of the words a weight between the 1 and 0. There's a similarity function. This time it's this if you remember from before. But this is the cosine coefficient. OCOM5204M – Data mining and text analytics © University of Leeds 15 of 26 And the cosine coefficient, it has only two terms involved. Then if we have, for example, document one has a lot of T1 and a little bit of T2 and document 2...the query, sorry...the query has a lot of T2 and a little bit of T1, then this angle there, the angle is equivalent to the cosine of the angle is equivalent to this equation. So, if you can calculate this, then we have a cosine coefficient. And the point is given a document and a query which overlaps, if the weights of the words are in the same proportions for the query and the document, then that angle is 0. And it turns out that the cosine of 0 is 1, so that's a perfect match. On the other hand, if a document has only got T1 in it and the query has only got T2 in it, then that angle is 90 degrees. And this calculation works out. There's cosine of 90, which is 0. There's no overlap between the document and the query. So, that's why we use this cosine function. So, let's go back to our query. I must have pudding. I'm quite keen on jam, and I definitely don't want traffic and lane. And I'm very keen on treacle. Well, the first recipe, which was the jam pudding recipe, it's got lots of pudding and lots of jam in it. It doesn't have any treacle. It doesn't have any lane. It does have a minor mention of treacle. So, it doesn't have any pudding, doesn't have lane, have a minimum of treacle. So, this jam pudding recipe maybe in it somewhere it says, get your jam and get your pudding. But if you haven't got any jam and you could get some treacle instead of jam, then use that whenever we say jam. And then you get your jam, and then you get the pudding recipe. And you mix it all up together, and there's your jam pudding. That's maybe what it says. So, we do the calculations. That's 0.8 times 1 plus 0.8 times 0.6 plus 0 times 0 plus 0 times 0 plus 0.2 times 0.8. And that gives us the sum of these multiplications. That's 1.44. OCOM5204M – Data mining and text analytics © University of Leeds 16 of 26 And then the sum of the squares is 0.8 squared plus 0.8 squared plus 0 squared plus 0 squared plus 0.2 squared. That's 1.32. And the sum of the query weight squared is 1 squared plus 0.6 squared plus 0 squared plus 0 squared plus [INAUDIBLE] squared plus 2. And then you put these terms into this equation. That's 1.44 divided by the square root of 1.32 times 2. And the overall answer is 0.89. If you want to, you can do this calculation on the back of envelope yourself. Or you could just take it for granted that I've done the right maths here. So, that's the answer. The jam pudding recipe gets a score of 0.89. Well, let's look at the next document. The next document, this is the report on traffic lanes. And there's no jam. So, there's no pudding, there's no jam, and there's no treacle. That makes this calculation fairly straightforward. It's 0 times 1 plus 0 times 0.6 plus 0.9 times 0 plus 0.8 times 0 plus 0 times 0.8. That's just 0. 0 divided by some other stuff which we can calculate very quickly. But actually it's 0 divided by the square root of blah, blah, blah, which is still 0. So, this is very efficient. If you get 0 on top, then you don't have to calculate all this other stuff. It's just 0. So, this Department of Transport report, even though we're now using weights, it still has a 0 score. What about the third one, this third one, which was a radio report on traffic jams in Pudding Lane? Remember, we don't have any bigrams. We simply have the unigrams. And there is a unique...there is some occurrence of pudding and some occurrence of jam because there's lots of traffic jam. So, jam appears quite a lot in here. In other words, the 0.6 times 1 plus 0.9 times 0.6 plus 1 times 0 plus 0.6 times 0 plus 0 times 0.8. At least the first two terms, pudding and jam, do have some significance here. So, it is 1.14 divided by 0.6 squared plus 0.9 squared plus 1 squared plus 0.6 squared plus 0 squared. That's 2.53. And the square of the query terms is 1 squared plus 0.6 squared plus 0 squared plus 0 squared plus 0.8 squared, which is 2. OCOM5204M – Data mining and text analytics © University of Leeds 17 of 26 So, this gets the score of 1.14 divided by the square root of 2.53 times 2. And that gives you a score of 0.51. In other words, the trade your traffic jam, radio traffic report of traffic jams in Pudding Lane, gets does get a non-zero score if I'm searching for pudding and jam and treacle because there are jam and pudding in this document. So, now we collect the results. And I'm just saying we found two hits. We have a similarity score for each of these, which lets us rank them the best hit is the jam pudding recipe. The second-best fit is a traffic report about traffic jams in Pudding Lane. So, let's have a look. The Boolean model is much simpler. There's a precise semantics. So, it's faster. But actually, it's not that useful because it doesn't rank the results. We've eliminated all but the one perfect match. Everything else is thrown out. Whereas the Boolean model, it does work for bibliographic systems. So, if you go to Leeds University library and search for books written by Eric Atwell, for example, then you'll find exact matches, and you won't find probabilistic matches. It doesn't really work for very large web resources. And furthermore, users find Boolean queries hard to formulate. If you've got to put AND’s and OR’s in, it gets difficult. There are fuzzy set models and extended Boolean models, but they don't really work too well either. The vector model is still quite simple. And its results show that it leads to fairly good results because you can have partial matching. So, some documents which aren't perfect still get a score and that then lets you rank the output. This is why it's popular with search engines. It's still unrealistic in some of its assumptions. It assumes term independence. So, we're still looking for traffic and jam without realising a traffic jam can go together, or we're looking for pudding and jam but not realising that traffic jam isn't an appropriate pudding. OCOM5204M – Data mining and text analytics © University of Leeds 18 of 26 Phrases and qualifications are not really taken care of properly. Again, there are generalised vector space models which can take into account bigrams. But they get more complicated. We haven't really talked about where these index terms come from. In our very simple model, I just said the only words we're interested in are pudding, jam, traffic, lane, and treacle. And we ignore the other ones. But actually, in real life Google has to allow for all the words in the documents as being possible search terms, and that can get to be a very large number of words. Also, where do we get these weights from? Well, I just said it's something to do with the importance of a document, the importance of the words within the documents. Well, Google has got some quite sophisticated mathematics for working out weights of words within documents and all these other weights too. How well do these systems actually work for practical applications? Well, that's the difference between Google and Yahoo and Bing. They have very slightly different weighting formulae. And they try them out on different populations. Google may try. Let's try this different weighting. We'll try it out on California first. And if the people of California seem to like it, then we can use it, extend it to the rest of the United States and then the rest of the world. That's basically what they do. They keep doing experiments by trying out variations of things. We'd also like to integrate information retrieval into more traditional database queries, question answering systems, and so on. So, nowadays if you type into Google some queries, it will find matching documents. You can also type into Google a proper question like, who is the President of the United States? And Google will recognise this as a question, and it will look into Wikipedia or other data sources to find answers. OCOM5204M – Data mining and text analytics © University of Leeds 19 of 26 We still haven't dealt with...so far, we've assumed a search term is just a unitary term. But what happens if I want to search for football and I also want to find web pages that have the word soccer in them? Soccer is the American word for football. At the moment, the model doesn't really do that. It has a separate index term for football and soccer. So, we'd like to be able to search for one and find both. In America, a faucet is what we call in Britain a tap. If you want to search for one of these and find results of both, those are synonyms. There's also ambiguity the other way. So, tap in English can mean what the Americans call a faucet, the thing that water comes out of. But it can also be a bang or a sort of dancing where you tap on the floor a lot. That's quite different from tap. A word like tap can have different senses. Of the word L-E-A-D can be a metal, lead, or it can be a lead for taking a dog for a walk. So, an individual keyword can have several senses. Or alternatively, a particular sense can have more than one keyword. And you'd like to be able to somehow link those. Also, the context that the word appears in may be important. So, for example, if I'm looking for articles to do with football, they may mention names of players and clubs and things but not actually the word football because it's taken for granted that the document is about football. So, there's no need to use the word football. And also, within football documents, the word goal would probably mean putting a ball into the back of a net. Whereas in a management document, the word goal will be the objective or aim that you're trying to reach. And as I said before, we have this problem of multi-word terms. The Venetian blind is not the same thing as a blind Venetian. A blind Venetian is someone from Venice who can't see, whereas a Venetian blind is a particular sort of window covering. OCOM5204M – Data mining and text analytics © University of Leeds 20 of 26 So, how do we evaluate these information retrieval systems? Why is Google better than Yahoo or Bing? Well, one is that there's all sorts of metrics, not straightforwardly accuracy at all, but other things like effort. How difficult it is for users to formulate a query which gets what they want? Or time, how fast is Google in producing its results compared to Yahoo, let's say? The other thing is presentation of the output. So, Google prides itself in presenting its results very straightforwardly, very clearly without much graphics. Whereas Bing, for example, has decided it's useful, that users actually like to have some graphics included. Coverage of the collection, how many web pages are actually included of all the web? So, Google has prided itself on trying to find every web page out there. Then we have accuracy, recall, precision. Those are metrics for measuring the results that you're given. And there's also things like user satisfaction. How satisfied am I with the results? Assuming that what you've got is not perfect, how do we get better? Query broadening is essentially trying to find more of what you want and less of what you don't want. A user is typically unaware of what is in the web pages in the collection or how to formulate the query which best finds what you want. So, query broadening is an approach which takes the query that the user gave...a naive query, not necessarily the best possible query...and tries to find a better one which gives you more of what you want by either finding new index terms that the user hadn't used or adjusting the term weight to get more of what you want. So, how does the system do this? Well, there's two sort of general approaches. One is to ask the user for some feedback. Tell me of the results I've given you, click the ones you like and don't click the ones you don't like. Another method is to offer the user a thesaurus or a term bank to broaden the query themselves. So, let's look at user feedback versus relevance feedback. Let's say the system gives you a set of all the hits. And of these, the user decides this one and this one are relevant and the others are not relevant. OCOM5204M – Data mining and text analytics © University of Leeds 21 of 26 Then the idea is, given the query, you replace it with a new query. And the new query Q dash is some constant alpha times the original query, so assuming the original query was reasonably good. So, we'll take that into account. And then you also add on some constant beta of the sum of all the good hits, the relevant hits. Assuming that the query is a vector, and the document is also a vector you basically take a fraction of the original query, and you add on a sum of all of...sum together all the good documents and take some fraction of those. And you can take away all the bad documents added together by sum weight of that. So, this essentially comes up with a new query which moves the query vector closer to the centroid of the relevant, retrieved document vectors and further away from the centroid of a nonrelevant, retrieved documents. So, you kind of have a new query, which is like the old query but has got a bit more of the good hits and a bit less of the bad hits in it. And the point of this, we expect that documents that are similar to one another in meaning to have similar index terms. So, the system creates a replacement query based on the initial query but adds index terms and weights that have been used to index the relevant documents, increasing their weights and also reducing the weights of terms found in non-relevant documents. So, how could this help? Well, if the user uses the word jam and if some of the recipes that are actually relevant have the word jelly in them, then jelly turns out to be a new term that wasn't used in the original query. But a lot of documents have got jelly in them. Therefore, we start adding a small weight of jelly in. It happens that the American word jelly is equivalent to the British word jam. And now hits can use jelly as well as jam. The other way around, if a user wants to have documents about lead and it gets documents related to dog walking because it has lead in them, so you've marked those as not relevant. And you downgrade all the other words, which appear in taking a dog for a walk. For example, dog and walk will be reduced. And therefore, other documents which have got lead in them, which also have dog and walk in them will be downgraded in future. OCOM5204M – Data mining and text analytics © University of Leeds 22 of 26 It may be that actually we want to set gamma to 0, ignoring non-relevant hits. So, we only have positive feedback. Because quite a lot of the hits may be non-relevant, that adds some sort of noise to it. The basic feedback mechanism is the user is invited to tick the ones that they like. And then for those ones only, you add in some extra weights, or you increase the weights on queries, word terms, which appear in those documents. This feedback formula can be applied repeatedly. So, you give the users some more results and again say, which ones are relevant? Typically, you only do it once, but you can do it repeatedly. This is particularly important for high-use systems. For example, we did have a PhD student who worked part-time in the British Library finding documents. His job was to do human information retrieval. However, one drawback is it's not fully automatic. The user has to specify, I like this one and this one and I don't like those. Let's get a simple example going back to the documents. Now we've got four documents rather than three. Our vocabulary is still the same. We want pudding, jam, traffic, lane, and treacle. The first document, which is a recipe for jam pudding, has got jam and pudding and a little bit of treacle in it. The second document on traffic lanes has got lots of traffic and lane, but no pudding or jam or treacle. Then we have a new document we didn't have before. This is a recipe for treacle pudding. So, it's got pudding, but it doesn't have jam in it. But it has got a lot of treacle in it. And then the fourth document is this radio item on traffic jam in Pudding Lane. And then if you do all the calculations, I mean, the query initially, my initial naive query, is I want pudding. OCOM5204M – Data mining and text analytics © University of Leeds 23 of 26 And I'm quite keen on jam, so I haven't even thought about treacle at all at this stage. So, I definitely want jam pudding. And there's my query, 1 for pudding, 0.6 for jam, and 0 for everything else. If we do a calculation using the cosine coefficient thing, these are results. The first document scores 0.91. The second document, which is only about traffic lanes, gets a score of 0. The third document gets a non-zero score because there is pudding in it, even though it's treacle pudding. So, it gets a 0.6. Then the fourth document has got lots of jam, but traffic jam, and lots of pudding, but Pudding Lane. And that said, it's got quite a high score. If you only want the best two documents, then we get the recipe for jam pudding is the best. And the radio item on a traffic jam in Pudding Lane is the second best. And now we ask the user, which one is actually relevant? And the user says, the first one is relevant. But the second one is actually not relevant. We didn't want that. Now we use this information of non-relevance in a clever way. What we do is we say we want more of the relevant document and less of a non-relevant document. So, we will return two documents. So, the new query is setting alpha to 0.5 and beta to 0.5 and gamma, the negative feedback, to 0.2. We want 0.5 of the initial queries. The initial query was pudding and probably jam and 0 for everything else, so half of that. And then I want to add on the good document, the one we thought was relevant. And the good document, that was the jam pudding recipe, which had 0.8 for pudding and 0.8 for jam. But it also had a mention of treacle. So, it says something like, if you haven't got any jam, then you could use treacle instead. So, it does mention treacle. So, we get 0.5. We add on a half of this. And then we take away a small amount of a non-relevant document. That does have pudding and jam in it, but we're also taking away treacle...sorry, also OCOM5204M – Data mining and text analytics © University of Leeds 24 of 26 taking away traffic and lane. We're not taking away any treacle because there's no mention of treacle in this. Now do this calculation. You end up with a new query. This new query is quite a lot for pudding, quite a lot for jam. It's got a negative value for traffic and lane. But now it has a very small positive value for treacle. So, treacle is also good because of this 0.5 of adding the good document. So, now we've got a new query. And now we feed this new query into our cosine coefficient to come up with for each of these documents a score with a new query, with this new query we feed it in. And the new scores are 0.96 for document, 0 for the traffic lane thing, 0.86 for the treacle pudding recipe because it's got pudding and it's also got treacle in it. And the traffic jam in Pudding Lane is still there, but it has a lower score because you've got downgraded values for these things. So, now if we ask for the top two hits, the top one is still the jam pudding recipe. But the second one is the recipe for treacle pudding. So, this giving feedback event gives us a new query which is better than the initial, naive query because it upgrades new terms that we haven't thought of, but which appeared in the good documents. So, the first document and the second one is also relevant. So, that's query broadening, giving user feedback. Another method for query broadening is to use a thesaurus or ontology. A thesaurus is a list of words and other words which are related to it. It's a controlled vocabulary of terms or phrases in a particular topic. It has classes of synonyms, words which mean more or less the same thing. And we also have a hierarchy defining broader terms, or hyponyms, and narrower terms like hypernyms. So, for example, Word Net or Roget Thesaurus is has got this. And you could also have this in medical terminology. We have something called SNOMED, or the standard nomenclature for medicine. So, what we can do this is...so for example, let's say if we want OCOM5204M – Data mining and text analytics © University of Leeds 25 of 26 to replace words with documents and query words with synonyms from a controlled language, this can improve precision and recall. So, for example, in a document, we have any old keywords. And we change the keywords into a standardised term using this thesaurus. And also, if you use a query, you can have any old words. And the user query is standardised to give us a normalised query. And the normalised query has a better chance of matching the normalised index terms in the content. So, for example, let's say the document contains data processor, and the query is electronic computer. Well, the thesaurus says a data processor is computer sense one. And the query electronic computer is also computer sense one. Therefore, there is a match. If we replace the term in a document with a controlled language and replace the term in a query with the controlled language, then we have a narrower term, which gives us better scores, increases recall and precision. So, that's another way of doing it. So, you can normalise the queries and terms in the query and terms in the document. And that potentially can be done automatically. I think we'll stop there. Here's some questions for you to think about. You should know by the end of this why a traditional database is unsuited to retrieval of unstructured information. It's just too complicated. SQL doesn't let you ask for documents which contain A or B or C. You should remember that you can't actually specify Boolean queries like A or B or C and not D. You have to convert it into disjunctive normal form. There is a way of doing that. But in principle, you don't really need to know how to. But you should have a go for simple examples. The matching coefficient used for weighted similarity function always has a value between 0 and 1. And the similarity of A with itself should be 1. That's because the similarity function is equivalent to the cosine, and cosine for any angle is between 0 and 1. And the cosine of similarity between something and itself is equivalent to an angle of 0, and the cosine if the angle is 0 is 1. OCOM5204M – Data mining and text analytics © University of Leeds 26 of 26 So, you should have a clearer understanding of the difference between the vector model and the set theoretical model in terms of power of representation of documents and queries. The set theoretical Boolean model is much simpler. But it doesn't really allow ranked ordering of results, or it doesn't really allow for query broadening to improve the results. So, that's information retrieval in a nutshell. We've looked at the difference between information retrieval and database querying through SQL. We've looked at the inverted file as a way of doing much more efficient matching in information retrieval we've compared the Boolean or set theoretical model against the weighted vector model. And the weighted vector model is much more like the thing that Google actually works with because it allows you to rank order the results. And then once you've got a rank order, you can even give feedback about which results are good and give you a better query to give more of what you like and less of what you don't like. We've gone through some worked examples, so you can see some of the maths behind it. If you want to do more by looking in the textbook by Jurafsky and Martin, they've got more examples for you to play with. You see, an evaluation, well, there's a number of different metrics. You can measure precision and recall of results. We can measure other things too. And you could do query broadening to improve the matches. And I think I'll stop there. So, thank you very much for listening and goodbye. [END] 
-----------------------------------------------------------------------------------------------------------------------------
OCOM5204M – Data mining and text analytics © University of Leeds 1 of 5 Multi-word expressions Tom Pickard: Hi. I'm Tom Pickard. I'm a research fellow at the University of Leeds, in the School of Computing, and the main thrust of my research at the moment is a project called the EDUBOTS project which is an EU-funded project looking at chatbots and conversational AI in higher education. So we're specifically interested in how chatbots, conversational agents, can be used to help teachers and students and support staff in universities deliver the best quality teaching and experiences that we can for students. I'm going to talk about Multi Word Expressions, which is essentially a linguistic phenomenon, really, but I'm going to talk about them in the context of the consequences that they have for natural language processing with computers. So, first of all, what are Multi Word Expressions? Essentially a Multi Word Expression, or an MWE, is a group of more than one word which functions linguistically as a single unit of meaning. So I've got some examples on the screen here of things like the phrase red herring, or spill the beans, a pain in the neck-- which are phrases which we understand and we are familiar with as a unit of multiple words, rather than just as single individual words. I've got at least one example here, l'espirit d'escalier, which is French, so this is not specifically a phenomenon in English. It's common to most if not all natural languages that are studied in the world, and that includes constructed languages like Klingon and Dothraki and elvish and things-- they also tend to create and include Multi Word Expressions. And not only do these things appear in lots of languages, but they're also relatively common as well. You may not be consciously aware of it, especially if you're speaking your native language, but they're very, very frequent. So up to about half, depending on who you ask and when they go and look, of the entries in word net, which is essentially a dictionary with some extra information in it, consist of more than one word. And this paper that I've cited there from Schneider, they looked at a particular corpus of, I think it was, reviews or comments from the internet, and they found well over half of sentences that were used contained at least one of these Multi Word Expressions. So they're frequent and they're a big deal. And they occur in lots of languages. One of the properties that Multi Word Expressions have is something called non-compositionality, which is, effectively, that, even if you are familiar with each of the component words of an expression, like one-armed bandit-- you know the number one, you know what arms are, you know what bandits are-- unless you've been given the meaning of that phrase as a whole unit specifically, you don't know that that means a slot machine or a gambling machine that you might get in a pub or a Casino. You just have to-- that's external knowledge, that's independent of the meaning of the component words. That's not just a yes or no thing, so some Multi Word Expressions are easier to understand than others. Or some you can sort of get some of the meaning. It kind of lies on a bit of a scale like I've got here. So something like the phrase Iron Curtain or Pandora's box, are very opaque-- they're almost impossible to understand without the extra context, even if you know what the word iron and the word curtain mean. Whereas a swimming pool or a fairy tale, you can get the gist of what those expressions mean. Or swimming pool is very easy to understand, if you know what swimming is and you know what a pool is. So we can imagine a foreign visitor, or maybe an alien, or a robot which has OCOM5204M – Data mining and text analytics © University of Leeds 2 of 5 perfect knowledge of every individual word in a dictionary, but doesn't know about these Multi Word Expressions, and they will very quickly run into trouble if they try to interact with users of the language. So, as you might be able to understand and suspect, quite a lot of people here probably speak English or at least one of the languages as a second language, you've probably encountered some of these when learning languages. So Multi Word Expressions do tend to cause problems for language learners. And also they're difficult for natural language processing. The instinct, when you're getting a computer to process language, is just to break up the text into individual words, languages which use words in the same way, and look at them separately and then construct the meaning. But Multi Word Expressions give you a way in which that doesn't matter. And in particular, of course, if you think of machine translation as the combination of learning a language and getting a computer to process the text, you're effectively trying to teach a computer to work from one language to another. These become very, very difficult because the same Multi Word Expression-- there may not be an equivalent one in the target language as there is in the source, and so on. So they get really difficult. So I'll start talking a bit more about how we actually go about handling Multi Word Expressions. The first thing to note is you can't just create a list of all of the ones that exist in a given language, and just refer back to it and then solve the problem that way. They are-- it's an ongoing problem of needing to identify or discover new Multi Word Expressions. People coin Multi Word Expressions frequently, especially if you're looking at social media corpuses, and Twitter, that kind of place. These examples I've got on the screen here for fake news, social distancing, chef's kiss-- a very recent coinage it is, but very, very familiar hopefully to anyone who's on Twitter. So there's an ongoing need to find Multi Word Expressions, and then work out what to do with them. So the starting point really, for trying to actually identify these things, is-- we look for what we call collocation, so that is words which occur together, close together, in sentences, or right next to each other in a sentence, more often than we might expect them to, if we assume that words are just sort of randomly distributed through text. So if we're looking for something that is occurring more often than we might expect it to by chance, that's the kind of thing that would imply that a statistical measure of some sort is what we want to use here. And we want that statistical measure to account for the fact that some words are more common just in general than other words are. Conjunctions and words are very, very common. And there are loads of words, especially in a language like English, which you just don't encounter very frequently in text. And so you need to adjust for that. There are a number of different measures that people have come up with for doing this. I'm just going to very briefly introduce the idea. Don't worry about the maths, but the idea of Pointwise Mutual Information, or PMI, which is a sort of occasionally-used statistical measure for this. But essentially, the idea is-- so we look at some text, a big bulk of text, all of Wikipedia or a big chunk of the internet or something like that, ideally. We look for how often do we see a pair of words, x and y, fake and news, occurring together, compared with how often we see those words occurring on their own, individually. So news is a reasonably common word, probably shows up relatively frequently in English text, especially on the internet, I suppose. But the pair fake news is probably more frequent than we would expect it to be if we just looked at the frequency of the word fake and the word news. And we can do some maths using that and construct something like this Pointwise Mutual Information measure and then, when that number is big, that implies that these words are co-occurring more often than we'd expect them to, which tells us there's probably something interesting about the combination of the words. However, that isn't enough for them to constitute a Multi Word Expression. OCOM5204M – Data mining and text analytics © University of Leeds 3 of 5 There are pairs of words which occur together frequently, which don't actually make up a single unit of meaning. So I've got a couple of examples here like strong and coffee, or black coffee. The verb to kick will occur often with the object, a ball. That's just part of the way in which language is used. So you need to do something else beyond just finding these interesting co-occurrences to actually look for Multi Word Expressions. And one of the ways of attacking this, and something that I looked at for my dissertation research, to try to use semantic embeddings or word vector embeddings. Hopefully this isn't entirely new to you, but you can use tools like word2vec or GloVe, which are the ones that I use, there's various models, to try to capture, with maths as vectors, something about the meaning of a word. So you replace words with a vector in this large multi-dimensional space. And by doing so, through the way in which the model has been trained, we capture something about the meaning of the word, or certainly in the relationships between them anyway. OK, so if we're looking for non-compositional Multi Word Expressions, that is, we know that the meaning of the expression as a whole differs from the meanings of the individual component words. So if we're talking about the phrase green thumbs, the meaning of that phrase is very different to the meanings of the word green and of the word thumbs. If you add the word green and the word thumbs together, you shouldn't expect to get the meaning of the phrase green thumbs. So maybe we can use vector embeddings to help us find where this is true-- where the phrase as a whole has a different meaning to the components. So if we take the two separate words where they occur in some text and join them together into a single word or a single token, and then build some word vector embedding models, and then we can look at the vector that represents green thumbs, in this case. And the vector for the word green, the vector for the word thumbs, and look at the differences between them and see how similar they are and if they're less similar if these vectors are further apart in this multidimensional space. That tells us that our phrase is less compositional, and therefore, that we should treat it as a Multi Word Expression. So the intuition is that we should be able to look at pairs of words which are statistically interesting in our text, and see if the meaning of the pair differs from the meaning of the component words that make it up. So in order to do that, I took a relatively large corpus, I took the text from the English Wikipedia. Wikipedia is lovely, and allow you to just download all of the content from there on their extracts that they produce once a month or so. And because I was doing this as an MSC project and working on just my own personal computer, I ended up taking a 10% sample of sentences in Wikipedia, just to reduce the size of the space and give me something that was a bit more manageable, and then looked for these pairs of words, or actually pairs and triples of words, that occurred together. Calculated the co-location measure, like Pointwise Mutual Information, took the most interesting to the highest-ranked half a million items from on that list, and then divided them up into multiple batches and replaced the pairs or the triples of words with a single token, and trained vector embedding models on that text to try to capture the meaning of the potential Multi Word Expressions. I did this two different ways. I did it with a word2vec model and with GloVe. Hopefully you've encountered both of those at least briefly. And then for each of the models, compared the vector for the potential Multi Word Expression with the components, calculating the cosine similarity. And all of this was based on a methodology by Will Roberts and Markus Egg from a couple of years ago. And the idea behind all of this was to allow me to compare word2vec and GloVe, which are different approaches to making vector embeddings, and see which one was more useful or more effective for this task. And my kind of hypothesis going in was, so Roberts and Egg did this with word2vec, maybe OCOM5204M – Data mining and text analytics © University of Leeds 4 of 5 GloVe will produce better results for some reason. It's a different way of constructing the word embeddings, maybe it will be better for this particular application. And it turns out, it's not. I found the opposite, in fact. So GloVe produced either lower quality results, so-- results where the ranking that I produced for how similar things were, and therefore how likely they were to be a Multi Word Expression, didn't correlate well at all with what humans thought of those same expressions. So there are reference data sets that you can use, where humans have been asked to evaluate how non-compositional an MWE is. Or you could get GloVe to give you similar results to word2vec, but the computational costs were higher, so I needed to run for longer, or took up more memory on my computer, that kind of thing. So from a practical point of view word2vec was the better option. I did also conclude that-- I mentioned earlier-- that I took a 10% sample of the sentences in English Wikipedia to work from, and that yielded very reasonable and quite effective results. 10% of Wikipedia was enough to give you a good idea of what would be-- of what was effective. You don't need to bother processing all of Wikipedia or something that large, and you can still get an idea of the results here. And there is a link here to code and my final data outputs, if anyone wanted to look at those and to share those with the community. And then there are some open questions, then, or some areas in which this could be sort of expanded upon, or looked into, in order to understand a bit more about this, or try to maybe push this idea a bit further. So I was working with Multi Word Expressions which were sets of groups of two or three words which occurred next to each other in the text. But there are other kinds of Multi Word Expression that we find in language, where there might be other words in the sentence or the structure might be more complicated. Could one expand this idea to try to look at those? Obviously I was working in English specifically, but you could take this idea and apply it to other languages and so on. It's obviously worth talking briefly about this idea of words and Multi Word Expressions with different languages. So English tends, for the most part, to separate words with spaces. It's a pretty good approximation, if you just look at English as a sequence of words with spaces in between them. But that doesn't hold true nearly as much for some other languages, so languages like German, and other Germanic languages, tend to combine nouns together into single compound nouns with lots of-- several words effectively sort of smushed together. So you need to worry about how to separate those first. And there are other languages like Chinese and Arabic and things, where just word boundaries sort of work very differently in the language. So there's lots of extra sort of challenges to just taking this method and applying it to a different language. It's potentially quite inefficient. It's quite expensive. I was training a word2vec model, or something like that, or 10 of them in fact, for every list of candidates that I was trying to explore. So there's potentially some ways that you could be done to improve the efficiency of such an approach. As I mentioned, it's an ongoing problem discovering MWEs, and you probably want to be able to do it reasonably efficiently. Something else I'm particularly interested in is looking at the way in which people use Multi Word Expressions in particular domains as well. So it's relatively easy to get hold of lots of general text for English. So you can grab things from the internet or Wikipedia or social media, and you'll get a general sense of how people use English just in conversation. OCOM5204M – Data mining and text analytics © University of Leeds 5 of 5 But if you're interested in the way in which Multi Word Expressions function, or even just the Multi Word Expressions which exist in quite specific domains to specific topics of text, like academia, or like data science modules, or medicine is a really good example-- it's harder to get those specialised text corpuses and work with those. And there's a big question for a lot of word embedding models and things, about how well do they transfer? They're often built on very large data sets of general language, how well do they transfer and apply to those more specific applications? And finally, how do Multi Word Expressions interact with more complex or more contemporary word embedding approaches like BERT. So I think Eric mentioned BERT at some point earlier in the course, that's a different kind of neural network for learning the word embeddings, and I'm interested in trying to maybe explore, A, does this same kind of approach affect the way BERT works? Does telling it that a pair of words is actually a single unit of meaning, then make it-- allow it to process that word better? Or does something about the architecture, and the way that BERT already works already give it some of that capability? And if the latter is true, if it is already capable of handling Multi Word Expressions to some extent, then can we flip the problem on its head, and can we use BERT-- or by investigating its architecture and its weights, can it tell us about Multi Word Expressions which it encountered in the text. Is there something in the structure of BERT that says, Oh, hang on-- green plus thumbs together means something different from green and thumbs. So that's a potential area for research, which I'm not familiar with any sort of detailed work that's already been done. And then the other thing I've got is a bit of suggested reading if anyone is interested in finding out a bit more about this. And the first thing I was going to recommend here is not so much reading as viewing. There's an episode of Star Trek-- The Next Generation which, I don't know if somebody might have already seen, called Darmok, which is not only one of the best episodes of Star Trek that's ever been made, but it's also a really, really good expression of-- an example of Multi Word Expressions and the challenges that they provide for both for humans trying to kind of make themselves understood and work in multiple languages, but also for machine translation, trying to handle the same problem. There's a very readable paper by Sag and his colleagues called Multi Word Expressions: A Pain In The Neck for NLP, which really sets out, from 2002, really sets out the sorts of problems that people working on Multi Word Expressions have been trying to solve for the last 10 years or so. And then if you really want to dive a bit more deeply into them, or especially if you were interested in looking at something in MWEs for a project, I would recommend Carlos Ramish's book. Carlos Ramish organises lots of conferences and things on Multi Word Expressions, and his book covers a lot of detail about how going about discovering and working with them. And then finally I've put in a little plug for myself, so there's a link there to my paper that I wrote last year describing these results and sort of sharing them with the community. [END]
-------------------------------------------------------------------------------------------------------------------------------
OCOM5204M – Data mining and text analytics © University of Leeds 1 of 9 Deep learning of text understanding: Google’s BERT Professor Eric Atwell: Hello, In this lecture, we're going to be looking at BERT. BERT is a Bidirectional Encoder Representations from Transformers. A method and tool kit and dataset and resource from Google Research Labs for understanding meaning relationships between sentences or rather understanding measuring the similarity in meaning between two sentences. It can be used for many tasks, which involve pairs of sentences which are related in some way. And the reference for the paper by Jacob Devlin and others. BERT, pre-training of Deep Bidirectional Transformers for Language Understanding. Notice, it got the best paper award at the 2019 conference NAACL. That's the North American Chapter of the Association for Computational Linguistics, the main professional body for computational linguistics researchers. And notice also, this is not just academics in universities but also in research labs. So Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova are all from Google's AI Language Research group. And it makes you think, perhaps, university research teams can't really compete nowadays in AI research, and in particular, in computational linguistics research, with the likes of very large and well-funded research labs at Google, Microsoft, Facebook, Amazon, and so on. However, because Google has this aim to make things useful to everybody, we have a benefit that they tend to make these computational linguistics resources freely available for other people to use. OK, so here's the overview of what's coming up. First of all, the general idea of BERT is that you have some pre-trained neural network representations learned from unlabelled text, in principle, unlabelled text. And then once you've got this language model, general language model of English, then you can fine-tune it for specific tasks like answering questions or translating sentences into another language. And the clever thing, one of the clever things about BERT is it does a lot of unsupervised learning of morphology, that's, word structure within words, syntax or grammar, and semantics or meaning. Unsupervised in the sense that there isn't a lot of information in terms of data, which is already labelled with the correct answers. But we'll see that they've cheated in a way. In a minute, we'll see that it's basically unsupervised learning. And at the heart of this, in terms of learning word segmentation, they use a method called WordPiece, which is unsupervised machine learning of text segmentation into pieces of word. Words and pieces of word, I should say. And then they predict sentence meaning from WordPiece meanings by joining together the WordPiece meanings, and they assume that in running text, two sentences next to each other should be related in meaning and that basic assumption allows them to take, pluck, pairs of sentences which they have related meaning, and then take one sentence and another sentence at random from the corpus, and assume it's not related meaning. And that gives them labelling, so the results are the general model for measuring the distance between two sentences. And then, this could be fine-tuned on specific human-labelled natural language processing datasets to learn the specific task of that dataset. And then they made a lot of experiments and lots of tables of results, which basically show that both BERT Base, a little version of BERT, and BERT Large, the much larger neural network, outperform all the systems on all tasks by a substantial margin. And for the same pre-trained model can successfully tackle a broad set of natural language processing tasks. And we'll see some of this, and the paper also, and it's got a lot packed into it. OCOM5204M – Data mining and text analytics © University of Leeds 2 of 9 It's also got ablation studies. What happens if you deliberately break bits of BERT and see how well it works in disabled mode? It's also got a huge set of references, some very detailed appendices of lots more information. And then finally, I'm going to finish with some questions about what isn't in the paper, which we might want to look at. OK, so there's a lot coming up in a nine-page paper plus some appendices and references. OK, so the first idea overall is that BERT is about learning pre-trained representations from large amounts of unlabelled text, and then this representation is a representation of whether or not two sentences are related in meaning, and then you can fine-tune it for very specific tasks. So BERT stands for Bidirectional Encoder Representations from Transformers, so the transformers are the neural networks, and the representations, the things you learn, so they are essentially data and not software. So don't think you can download the BERT.pui program and then run it. Although actually, there is a BERT program as well. So if you go to the website, they do have software as well, which uses the BERT models or trains the BERT models. The next idea is of pre-trained representations from unlabelled text. Now the idea is that it's a general model of English word structure and grammar. And for pre-training, they use a huge corpus, 800 million words from the Google Books corpus. If you go to books.google.com, this is a very large collection of books scanned in from Oxford library, Oxford University library and many other University libraries. So there's a very large collection, mainly learned texts, but actually anything that's in university libraries. And also, English Wikipedia, which is an even larger and growing collection of English language text. So it's English. It can be fine-tuned to create models for a wide range of tasks. And tweak for specific language tasks which involve pairs of sentences in some way. And for state-of-the-art results on 11 natural language processing tasks. They give quite a few examples of specific tasks, and each of them they do very well. Now, the code and the pre-trained models are all available from the Google Research BERT GitHub website, so you can have a look yourself. Beware that the method for learning a model takes very large amounts of processor and memory. But once the pre-trained model, that they once have learned it, that can be plugged into simpler tasks, and you can run it on a laptop, if you don't mind waiting a few hours, or some time, let's say. OK, so the core linguistically about this is unsupervised machine learning. Unsupervised machine learning of morphology, that's word structure, syntax and grammar, or semantics as well. So it's a big neural network, a transformer neural network. So if you do a deep learning course, you may learn something about transformers as a particular structure for neural networks. I'm not going to go into details here. There is some more detail in the Jurafsky and Martin textbook. But essentially, it's a way of having inputs and outputs, and mapping not just a sentence into a class, as a classifier might do, but a pair of sentences and a class. So in training, you show BERT many examples of sentence and sentence, and the result is yes or no. And, yes, are they related in some way? And test is also given a new sentence plus another sentence. BERT will predict yes or no. So BERT is learning sentence plus sentence plus class, the yes or no, whether or not they are related. Without any linguistic theory in the model, there's no explicit rules about word structural morphology, grammar or syntax, or even meanings. Instead, you use a range of unsupervised or semi-supervised machine learning models for morphology, syntax, and semantics from a large text corpus. This should remind you of the Morpho Challenge you've heard of before, which was unsupervised learning of morphological analysis, segmenting words into OCOM5204M – Data mining and text analytics © University of Leeds 3 of 9 morphemes. Originally it was developed for highly complex languages like Finnish or Turkish, where words could be quite long, consisting of several morphemes. But this also works for English. OK, and you also should remember word2vec. This is an unsupervised measure of learning semantic vectors for meanings of words or morphemes. The idea being that the meaning of a word if you have a reasonably common word, like cat, you look up thousands of examples of cat in a corpus, and take the words on either side. The context of the word cat, and those contexts are words which appear and essentially those words give you the meaning of cat. So the meaning of cat is similar to the meaning of dog if the context vectors of cat are similar to the context vectors of dog. And you also, on top of that, you have this unsupervised learning of sentence pairs being similar meanings, by giving lots of pairs of sentences which are similar, and then taking random pairs of sentences which are not similar, and then it learns to distinguish between similar or not similar. OK, so the first stage in all this is WordPiece, unsupervised machine learning of text segmentation into words and morphemes. First of all, the first stages to segment each of the sentences into bits, but not words necessarily. So linguistic theory tells us that there are words and morphemes, that words is made up of word plus plural s. That is a root and an affix. But that's just the theory, that only works if you have a dictionary, and that works for English, we have lots of dictionaries for English. But Google is very keen on methods which work on any language, even if there aren't lots of resources for it already. And that's why the Morpho Challenge was aiming to do Finnish and Turkish and other such languages because there aren't that many formal language learning linguistic resources for all these other languages. So that's one issue. Another issue is that in any corpus, there's always going to be more and more new words you haven't seen before, so how do you deal with those? There's rare words which occur very occasionally, and there's lots of out of vocabulary words, words you haven't seen before. Well, what WordPiece does, this is inspired by some other Google researchers working on machine translation, and in machine translation they did have to handle lots of languages where they didn't have as many resources, as for English. So WordPiece works pretty much for any language which is written in some characterbased method, and you set in advance how many WordPieces you want. What is the size of the vocabulary? And 30,000 seems to be a reasonable number. It turns out that a good English dictionary may have 30 to 50,000 entries in it. Base words or roots in it. So that's a reasonable number. So WordPiece is an unsupervised machine learning morphological analyser. What it does is it takes the entire corpus, takes out all of the words, and it tries to compute frequencies of words and pieces, such as, the common words are kept as pieces because they're quite frequent. For all the less common words, you try to chop it up into pieces such that the pieces are also common. So, for example, unsupervised may be quite rare, but if you chop it up into un, supervise, and duh, then supervise is quite common, and un is definitely common, and duh is also very common. So all the rare words are chopped into smaller parts, and then it tries to work out-- it tries essentially various possible permutations and combinations until it finds the best entropy combination, the one which has most reasonably low-frequency pieces. So this is reminiscent of Morpho Challenge, it wasn't part of the Morpho Challenge, but they did it separately at Google research themselves. So there's no more rare words or out of vocabulary words. There's only common word pieces. And for each WordPiece then just as in the style of word2vec, you input the text, only this time instead of dividing up into words, you divide up into WordPieces. And then, you build a vector representing the concordance context for each WordPiece. What are the OCOM5204M – Data mining and text analytics © University of Leeds 4 of 9 words or WordPieces which appear before and after it? And that gives you a vector of frequently occurring concordances or contexts. OK, so now we have a way of representing all the words, even the rare words because they're made up of WordPieces, and then we have to predict sentence meanings. The meanings of the whole sentence. Not just individual words. Well, it's not explicitly stated in the paper, but if you look into the code, you should find that the default assumes it's about 25 WordPieces per sentence. And if there's less than 25, well, sorry, if there's more than 25 words, then the default is just to ignore the rest of the sentence. Or if there's less than 25, so you fill it in with blanks. But that's surprisingly, surprisingly to me as a linguist, that seems to work OK. Anyway, then, so the semantics of a sentence is essentially joining the word meanings of each of the WordPieces, the vectors of each of the WordPieces, but joining in a rather sophisticated way. You don't just simply add them together or concatenate them. We do not use traditional left-to-right or right-to-left language models to pre-train BERT. We simply mask some percentage of the input tokens at random and then predict those masked tokens. This is a Masked Language Model, although it is often referred to as a cloze task So if any of you have learned English as a second language, you might have come across cloze tasks. This is a test of your English language ability. You're given a sentence, and one of the words is blanked out, and you have to predict what the word is. That's essentially what BERT tries to do. And therefore, it optimises the vector representing the meaning in terms of how good the best vector is predicting the missing word. So that you have a vector for all of the sentence, and then you take out one of the words or some of the words, and you see how you can optimise the vector to predict those taken out words. And this has to do with the details of a neural network, if you really want to find out more, there's a better explanation in the Jurafsky and Martin textbook, and you could try this with the code from the web page. OK, so then they've got a method then for representing meanings of-- well, first of all, for the chopping up the sentence, the text into WordPieces, and then representing the meaning of each WordPiece, and representing the meaning of a sentence as a combination of the WordPieces. Now, the next task is how do you represent relationship between two sentences? Well, they came up with a really neat idea that you can assume that in any coherent text, each sentence follows on from the previous sentence. If I'm writing a sentence, or writing some PowerPoint slides, or in Wikipedia, or in Books given that their training set was Wikipedia and Books, each of these documents, each Wikipedia entry is actually not just a collection of sentences at random, but it is first sentence followed by second sentence followed by third sentence followed by fourth sentence. And there is a meaning relationship. What the meaning relationship is a bit vague, but they are related in meaning. Otherwise, the whole text wouldn't make sense. So they came up with this idea for each sentence is related meaning to the next sentence. It's a bigram model for sentence meaning. So this is a way of using the text without actually explicitly labelling it. It is sort of human-labelled. It does make the assumption that the author labelled each sentence, each sentence as being related to the previous sentence. And then, if you take a sentence and another sentence at random from somewhere else in the corpus, then it's not related because it hasn't got the human label if you like. There's an implicit label between sentence one and sentence two saying they are related. An implicit label between sentence two and sentence three saying they are related in some way. So they're able to pre-train a Next Sentence Prediction task. In training sentences, A and B, 50% of the time, B is the actual next sentence that follows from A, in the corpus, labelled as, IsNext, in the Book, or in the Wikipedia entry. And 50% of the time, it is a random sentence from anywhere else in the corpus labelled as NotNext. So the input sentence A and OCOM5204M – Data mining and text analytics © University of Leeds 5 of 9 sentence B, from the pre-training, are related by IsNext. But this is very similar to, analogous to, sentence pairs in paraphrasing. So when you have sentence B is a paraphrase of sentence A, means the same thing as sentence A. Or also, in hypothesis-premise pairs in entitlement. Sentence A entails sentence B. That's another relationship. Or in question-passages pairing, questions and answers, the question and the answer are related in a similar way as to, sentence one is related to sentence two. And even in text to zero pairs, in text classification. If you want to have a text sentence, which is classified as A or B, but it's a bit like saying you've got a pair of sentences, but the second pair-- second in the pair is nothing. So all of these specific tasks are much like the general Next Sentence Prediction task. So you could take the general model and then fine-tune it by giving it an extra training set for this particular task. So it's all fed into an output layer for classification. So then, they went on to try this out on a number of hand-labelled natural language processing datasets and tasks collected by various computational linguistics researchers. As I've said before, a lot of computational linguistics research is done in conferences. You typically have people presenting their own papers, but they also have these shared tasks, where the organisers have got together a standard hand-labelled classification training set and test set. And anybody can enter. And you're given the training set, and the task is, can you come up with a classifier which predicts correctly? The test set. And you're not actually given the answer to the test set, but you're evaluated on the test set. And GLUE is like a super competition. So the GLUE researchers got together a number of existing datasets and put them together into a general language understanding evaluation set of tasks. So they didn't actually have to do any labelling themselves they got together several different ones, so that anybody can evaluate on all of these things. So SST-2 is a movie review. Given a movie review, can you predict is it positive or negative? Do they like the review or don't like the review? Well, CoLA is given a sentence. Is it linguistically acceptable? Yes or no? So both of these tasks have just got one sentence. So in BERT terms, you give a sentence and a null or blank for the second sentence, and the yes or no in the first sentence answers yes or no, is it positive? And the second one, yes or no, is it linguistically acceptable? So they're the same sort of tasks but slightly tuned from the original pair task. STS-B and MRPC are two other datasets. In this case, there are pairs of sentences, and the task is, are they similar in meaning? So this is much more like the original BERT training task. There's also the QQP, which is a pair of questions on Quora. Are the two questions semantically equivalent? Now, Quora is a website where anybody can post a question, but before you post a question, you should see has anyone else already posted a similar question. So this would be a useful classifier to do that. This is actually very similar to STS and MRPC, except the two sentences are both questions. Then we have QNLI, which is given a question and a sentence. Does the sentence contain the correct answer? So it's not really a question and answer, but rather is the answer within the sentence? Because sometimes you can have more than just the straightforward answer. But it is essentially question and answer in it's natural language inference. From the question, can you infer the answer? Then we have two more. MNLI and RTE. Given the first sentence and a second sentence, is there an entailment or contradiction? So does the first sentence entail the second sentence, or does the first OCOM5204M – Data mining and text analytics © University of Leeds 6 of 9 sentence contradict the second sentence? Or, as a third option, are they neutral? So each of these are, basically, is there a meaning relationship between a pair of sentences, but some variation on that? So those are all standard test sets within GLUE. And the Google researchers also tried to other quite large datasets. There's the SQuAD dataset, Stanford Question Answering Dataset, as pairs of questions and answers. But in this case, it's a bit more complicated, given a question and the other text is a text span in a Wikipedia passage. Is the answer in the span of Wikipedia passage? So is this the Wikipedia text contain the answer, and where is it? And there's also SWAG, which is Situations With Adversarial Generations. Given a sentence, choose how it finishes from four choices. So both of these are a bit more complicated, there's a sentence, and then the second piece of text isn't just a sentence. In SQuAD, it's a larger piece of text, and you have to try to predict where in there is the answer. And then SWAG, the second bit is four possible continuations and you've got to choose one of them. So it's a bit more complicated. But you can see that BERT, the underlying model, can be modified a bit or fine-tuned, as they put it, to fit any of these tasks. And they did try this, and they found that both BERT Base, the simple language model, and BERT Large, the larger language model, outperformed all of the existing systems on all of the tasks. They tried. So various other existing systems and BERT won by a substantial margin. And you can see the paper for what that margin actually is. And furthermore, BERT Large significantly outperforms BERT Base across all the tasks. It gets a better score. But it takes a lot longer. In terms of neural networks, the Base has got 12 layers, and the Large has got 24 layers, so it takes a lot more processor, a lot more time, and don't try these experiments on your laptop unless you want to spend a long time waiting. That's the main thing. In addition, a paper is only nine pages long. So if you submit to a conference, like this, typically they invite short papers of four pages or long papers of eight pages. And if your paper is accepted, you're given an extra page to add in anything else that the reviewers asked you to add in, so nine pages is about maximum. You can also add in references not counted in the nine pages and maybe an appendix if you really need to. So this paper has got references down in the appendix. Incidentally, for coursework, do please stick to the page limits, so that doesn't apply, as is these extras don't apply to your coursework. Anyway, they have to have-- so in addition to showing the Bert model, and showing lots of results to say how good it is, they also tried some what they call ablation studies. So this is a bit like in psychology, if you want to look at how the human brain works, then cut out bits of the brain and see what happens. You don't cut out bits of the brain but rather you study people who've had accidents, or for other reasons, parts of their brain doesn't work anymore and see how they behave and see what insights that gives you on normal brain behavior. In the same way, in neural network studies, people like to turn off bits of a neural network and see how this affects performance. And there's more on this in the BERTology paper that you can read as well. OK, the conclusion is that deep bi-directional architectures, that's a neural network architecture, where you don't try to predict the next word or try to predict the previous word, but try to predict the whole sentence using this cloze test architecture. This allows the same pre-trained model to successfully tackle a broad set of NLP tasks. The BERT works for all sorts of tasks which involve pairs of sentences being semantic related in some way. They've also got quite an extensive set of references, so they build on lots of other research. For example, the WordPiece research wasn't part OCOM5204M – Data mining and text analytics © University of Leeds 7 of 9 of this paper, but they were able to use that even though that of itself is quite a substantial novel piece of research. So if you want to find out, or get some overview of the current state-of-the-art in natural language processing research, just read all the papers in the references. You'll find that's quite a lot there. We also have quite a lot of appendices with implementation details and also references to the website where you can download the software more details on the experiments and the ablation studies. OK, so they managed to pack a lot into nine pages, plus references, plus appendices. But even so, there are some issues which they haven't gone into some detail, so you might want to think about this. First of all, there's the implementation details. If you actually want some code examples to try out yourself, then there is a URL, and you can go to the website and go to the Google archive there. They haven't really said something about-- this is a big issue in theoretical linguistics, is that long sentences don't mean the same as short sentences. I love you, obviously means something quite different from, I am very strong and bold, and I love you because you are tiny and unique. OK, but they haven't really discussed how they coped with different lengths. It simply says that they are input. A sentence is a combination of the word pieces. They also, they mention some of the computing resources required, but they haven't really said, and they did say, OK, to learn the underlying BERT model as particularly the Bert Large model requires huge computing resources. But once it's trained, then you can plug the pre-trained model into downstream tasks like question-answering, and this, to some extent, this sort of thing could be done on a laptop but you'll find they don't really talk about what bits of it work on smaller resources. Another thing they don't really do is that they talk entirely about the overall accuracy of a language model. They don't give what linguists and corpus linguists like to see is specific examples of sentences or pairs of sentences which work and pairs of sentences which don't work. And explanations or some discussion or analysis of why it didn't work. So that's in the BERTology paper, there's more discussion about that. They haven't really talked about what tasks in NLP don't suit BERT. This isn't just their problem, but this is very common in research papers, where they present some results and then say our results are really good. And they could also be used for lots of other tasks. But they haven't said what tasks are not suitable for BERT. So BERT, inherently, is a language model based on sentences from Books and Wikipedia pages, and it assumes that one sentence is linked to the next sentence in some sort of meaning relationship, and it does work with other tasks, which are something like pairs of English sentences, which are readily related in meaning. But not all of natural language processing is about pairs of sentences which are related in the meaning. It is also only about English, and they haven't even mentioned how this transfers to other languages. Having set this up and made the software available, other researchers have gone on to try it on other languages. So the example is Arab BERT, which is trained on a large Arabic text dataset and does the same thing with pairs of Arabic sentences. It assumes the WordPiece tokenisation, which is a clever way of capturing the meanings of common words, and then for rare and out of vocabulary words, they're broken up into pieces and, hopefully, the pieces are common. But it doesn't deal with multi-word expressions because the default assumption is that a piece is either a word or a bit of a word, but not a multi-word. And it doesn't. It's not clear how this works for languages other than English which are much richer morphology. Like Arabic, for example, the Arab BERT system also uses WordPiece, but maybe something other than WordPiece would be more appropriate for Arabic. OK, the final issue that doesn't really deal with much is how do we understand how BERT works? OCOM5204M – Data mining and text analytics © University of Leeds 8 of 9 For people like me who are trained in linguistics and computational corpus linguistics, at least, it seems counterintuitive that BERT can learn mapping between the sentence and sentence and class, yes or no, without any linguistic theory, any rules of morphology, any dictionary of words, and morphemes in English. Any grammar of English which says that words are nouns or verbs or adjectives or whatever, there's no semantic model which says that words can have more than one meaning, words have senses, or that two words can mean the same thing, there's nothing explicit about that. But there is in a dictionary. We just don't bother using a dictionary at all OK, so these are questions about BERT for you to think about, and if any of these questions grab your attention, then maybe you can use these as part of your research proposal. Or maybe for your project, later on, when you come to do a project, you could choose one of these questions as something you want to investigate further. This is well worth doing a longer project on. OK, so in summary, we've looked at BERT as a model for pre-trained representations from unlabelled text. And unlabelled, at least, except for in the sense that the assumption is that there is an inherent, implicit label between each sentence, say that the sentence one is related to sentence two, and sentence two is related to a sentence three, and also that sentence one is not related to any other sentence at random plucked from the corpus. So those are implicit labels, at least so. And then the idea is that it's learned, it's trained to learn that the two sentences are related, and then it can be finetuned on specific relationships, like sentence one is the question and sentence to is the answer or sentence one is contradictory to sentence two. OK, then we have, and it's fine-tuned by giving some training examples with that specific relationship, and then you test it on some test set with the same relationship. And it does unsupervised learning of morphology using WordPiece. Unsupervised learning of grammar and semantics, inherently, by this Word Piece, unsupervised learning of text segmentation. And then, it predicts sentence meaning by combining WordPiece meanings, but using this cloze test and model, and then it assumes adjacent sentences have related meanings, and then it can be fine-tuned on hand-labelled sentences, where the hand labelling says what is the relationship between the two sentences. The results are quite astounding, and this is why everyone is using BERT. That both BERT Base and BERT Large outperform all the other systems they tried on all of the tasks by a substantial margin. The same pretrained model can successfully tackle a broad set of natural language processing tasks. And on top of all this, the paper also has some ablation studies looking at what bits of BERT are actually contributing what functionality? There's a large set of references giving you a broad reading for the current state-of-the-art research in natural language processing. There's some detailed appendices with explanations of the experiments in more detail. And finally, I presented some examiners some questions we're not so sure about BERT from the paper, at least, that you might want to follow up, in further research. OK, I hope you've enjoyed this. Notice, in the summary of BERT paper, I've not talked much at all about the neural network architecture or the mathematical modeling. There's more of this in the Jurafsky and Martin textbook. But actually, probably if you use BERT, then you don't-- I mean, they built BERT as a black box. They made it available, at least, as a Black box that you can use on other tasks. And that the beauty is you don't necessarily have to understand the neural network model unless you want to become a machine learning researcher, developing variations of neural networks. So if you actually want to build new neural network architectures, then you have to do that. But for that, I would advise you to take on the deep learning course and get more on the deep learning architectures within that module. OCOM5204M – Data mining and text analytics © University of Leeds 9 of 9 OK, thank you for listening, and goodbye for now. [END]
